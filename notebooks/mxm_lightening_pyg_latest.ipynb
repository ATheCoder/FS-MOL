{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "FS_MOL_CHECKOUT_PATH = os.path.abspath(\"../\")\n",
    "\n",
    "os.chdir(FS_MOL_CHECKOUT_PATH)\n",
    "sys.path.insert(0, FS_MOL_CHECKOUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "from fs_mol.clip_like import FingerprintEncoder\n",
    "from fs_mol.data.clip_dataset import CLIPDataset\n",
    "from fs_mol.modules.gat import GAT_GraphEncoder, TrainConfig\n",
    "from fs_mol.data.clip_fewshot_dataset import FSMOL\n",
    "from fs_mol.models.protonet import calculate_mahalanobis_logits\n",
    "import wandb\n",
    "from fs_mol.data.torch_dl import FSMOLHTorchDataset, FSMOLTorchDataloader\n",
    "from fs_mol.data import DataFold\n",
    "from torch_geometric.loader import DataLoader\n",
    "import atexit\n",
    "\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from fs_mol.utils.torch_utils import torchify\n",
    "\n",
    "atexit.register(torch.cuda.empty_cache)\n",
    "#config.dim, config.layer, config.cutoff, config.encoder_dims, 512\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TrainConfig:\n",
    "    # Training Settings:\n",
    "    batch_size: int = 64\n",
    "    train_support_count: int = 32\n",
    "    train_query_count: int = 256\n",
    "    train_shuffle: bool = True\n",
    "\n",
    "    temprature: float = 0.07\n",
    "\n",
    "    # Validation Settings:\n",
    "    valid_support_count: int = 64\n",
    "    valid_batch_size: int = 256\n",
    "\n",
    "    # Model Settings:\n",
    "    envelope_exponent: int = 6\n",
    "    num_spherical: int = 7\n",
    "    num_radial: int = 5\n",
    "    dim: int = 256\n",
    "    cutoff: int = 5.0\n",
    "    layer: int = 7\n",
    "\n",
    "    accumulate_grad_batches: int = 4\n",
    "    learning_rate: float = 1e-4\n",
    "    weight_decay: float = 1e-5\n",
    "\n",
    "    dropout: float = 0.2\n",
    "\n",
    "    encoder_dims = [128, 128, 256, 256, 512, 512]\n",
    "\n",
    "\n",
    "config = TrainConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint_weights(path):\n",
    "    checkpoint = torch.load(path)\n",
    "\n",
    "    return checkpoint[\"state_dict\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_dataset \u001b[39m=\u001b[39m FSMOLHTorchDataset(\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mpyg\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      2\u001b[0m valid_dataset \u001b[39m=\u001b[39m FSMOLHTorchDataset(\u001b[39m\"\u001b[39m\u001b[39mvalid\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mpyg\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m test_dataset \u001b[39m=\u001b[39m FSMOLHTorchDataset(\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mpyg\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/FS-MOL/fs_mol/data/torch_dl.py:41\u001b[0m, in \u001b[0;36mFSMOLHTorchDataset.__init__\u001b[0;34m(self, datafold, mol_type)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtasks \u001b[39m=\u001b[39m dill\u001b[39m.\u001b[39mload(\u001b[39mopen\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/FS-MOL/datasets/mxm_\u001b[39m\u001b[39m{\u001b[39;00mdatafold\u001b[39m}\u001b[39;00m\u001b[39m_task_list.pkl\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m     40\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 41\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtasks \u001b[39m=\u001b[39m dill\u001b[39m.\u001b[39;49mload(\n\u001b[1;32m     42\u001b[0m         \u001b[39mopen\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39m/FS-MOL/datasets/fs-mol-mxm/\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m datafold \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m_tasks.pkl\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     43\u001b[0m     )\n\u001b[1;32m     45\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDone!\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/dill/_dill.py:272\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, ignore, **kwds)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(file, ignore\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds):\n\u001b[1;32m    267\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[39m    Unpickle an object from a file.\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \n\u001b[1;32m    270\u001b[0m \u001b[39m    See :func:`loads` for keyword arguments.\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 272\u001b[0m     \u001b[39mreturn\u001b[39;00m Unpickler(file, ignore\u001b[39m=\u001b[39;49mignore, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\u001b[39m.\u001b[39;49mload()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/dill/_dill.py:419\u001b[0m, in \u001b[0;36mUnpickler.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\u001b[39mself\u001b[39m): \u001b[39m#NOTE: if settings change, need to update attributes\u001b[39;00m\n\u001b[0;32m--> 419\u001b[0m     obj \u001b[39m=\u001b[39m StockUnpickler\u001b[39m.\u001b[39;49mload(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    420\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(obj)\u001b[39m.\u001b[39m\u001b[39m__module__\u001b[39m \u001b[39m==\u001b[39m \u001b[39mgetattr\u001b[39m(_main_module, \u001b[39m'\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    421\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ignore:\n\u001b[1;32m    422\u001b[0m             \u001b[39m# point obj class to main\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/storage.py:241\u001b[0m, in \u001b[0;36m_load_from_bytes\u001b[0;34m(b)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_load_from_bytes\u001b[39m(b):\n\u001b[0;32m--> 241\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mload(io\u001b[39m.\u001b[39;49mBytesIO(b))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py:817\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    815\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    816\u001b[0m         \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m--> 817\u001b[0m \u001b[39mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py:1029\u001b[0m, in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1026\u001b[0m         \u001b[39m# if not a tarfile, reset file offset and proceed\u001b[39;00m\n\u001b[1;32m   1027\u001b[0m         f\u001b[39m.\u001b[39mseek(\u001b[39m0\u001b[39m)\n\u001b[0;32m-> 1029\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39;49m(f, \u001b[39m'\u001b[39;49m\u001b[39mreadinto\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mand\u001b[39;00m (\u001b[39m3\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m0\u001b[39m) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39mversion_info \u001b[39m<\u001b[39m (\u001b[39m3\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m2\u001b[39m):\n\u001b[1;32m   1030\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1031\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtorch.load does not work with file-like objects that do not implement readinto on Python 3.8.0 and 3.8.1. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1032\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mReceived object of type \u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(f)\u001b[39m}\u001b[39;00m\u001b[39m\\\"\u001b[39;00m\u001b[39m. Please update to Python 3.8.2 or newer to restore this \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1033\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfunctionality.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1035\u001b[0m magic_number \u001b[39m=\u001b[39m pickle_module\u001b[39m.\u001b[39mload(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_dataset = FSMOLHTorchDataset(\"train\", \"pyg\")\n",
    "valid_dataset = FSMOLHTorchDataset(\"valid\", \"pyg\")\n",
    "test_dataset = FSMOLHTorchDataset(\"test\", \"pyg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 14\u001b[0m\n\u001b[1;32m      1\u001b[0m train_dl \u001b[39m=\u001b[39m FSMOLTorchDataloader(\n\u001b[1;32m      2\u001b[0m     train_dataset,\n\u001b[1;32m      3\u001b[0m     batch_size\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mbatch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     query_count\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mtrain_query_count,\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     12\u001b[0m batches \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(train_dl))\n\u001b[0;32m---> 14\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m batches:\n\u001b[1;32m     15\u001b[0m     \u001b[39mprint\u001b[39m(i)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "train_dl = FSMOLTorchDataloader(\n",
    "    train_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    datatype=\"pyg\",\n",
    "    num_workers=0,\n",
    "    shuffle=config.train_shuffle,\n",
    "    support_count=config.train_support_count,\n",
    "    query_count=config.train_query_count,\n",
    ")\n",
    "\n",
    "\n",
    "batches = next(iter(train_dl))\n",
    "\n",
    "for i in batches:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dl_16 = FSMOLTorchDataloader(\n",
    "    valid_dataset,\n",
    "    batch_size=config.valid_batch_size,\n",
    "    datatype=\"pyg\",\n",
    "    num_workers=0,\n",
    "    support_count=16,\n",
    ")\n",
    "valid_dl_64 = FSMOLTorchDataloader(\n",
    "    valid_dataset,\n",
    "    batch_size=config.valid_batch_size,\n",
    "    datatype=\"pyg\",\n",
    "    num_workers=0,\n",
    "    support_count=64,\n",
    ")\n",
    "valid_dl_32 = FSMOLTorchDataloader(\n",
    "    valid_dataset,\n",
    "    batch_size=config.valid_batch_size,\n",
    "    datatype=\"pyg\",\n",
    "    num_workers=0,\n",
    "    support_count=32,\n",
    ")\n",
    "valid_dl_128 = FSMOLTorchDataloader(\n",
    "    valid_dataset,\n",
    "    batch_size=config.valid_batch_size,\n",
    "    datatype=\"pyg\",\n",
    "    num_workers=0,\n",
    "    support_count=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dl_16 = FSMOLTorchDataloader(\n",
    "    test_dataset,\n",
    "    batch_size=config.valid_batch_size,\n",
    "    datatype=\"pyg\",\n",
    "    num_workers=0,\n",
    "    support_count=16,\n",
    ")\n",
    "test_dl_32 = FSMOLTorchDataloader(\n",
    "    test_dataset,\n",
    "    batch_size=config.valid_batch_size,\n",
    "    datatype=\"pyg\",\n",
    "    num_workers=0,\n",
    "    support_count=32,\n",
    ")\n",
    "test_dl_64 = FSMOLTorchDataloader(\n",
    "    test_dataset,\n",
    "    batch_size=config.valid_batch_size,\n",
    "    datatype=\"pyg\",\n",
    "    num_workers=0,\n",
    "    support_count=64,\n",
    ")\n",
    "test_dl_128 = FSMOLTorchDataloader(\n",
    "    test_dataset,\n",
    "    batch_size=config.valid_batch_size,\n",
    "    datatype=\"pyg\",\n",
    "    num_workers=0,\n",
    "    support_count=128,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[10952], edge_index=[2, 23322], pos=[10952, 3], bool_label=[256], batch=[10952], ptr=[257])\n"
     ]
    }
   ],
   "source": [
    "batches = next(iter(valid_dl_32))\n",
    "\n",
    "print(list(batches)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:8yr5woq6) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2822f97bb6ee4d1aa82055ecc267071e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>128_valid_acc/dataloader_idx_3</td><td>▁▃▄▅▆▂▂█▅▇</td></tr><tr><td>128_valid_avg_precision/dataloader_idx_3</td><td>▁▃▃▄▄▃▇▇█▇</td></tr><tr><td>128_valid_balanced_acc/dataloader_idx_3</td><td>▁▃▄▅▆▂▃█▅▇</td></tr><tr><td>128_valid_delta_auc_pr/dataloader_idx_3</td><td>▁▃▃▄▄▃▇▇█▇</td></tr><tr><td>128_valid_f1/dataloader_idx_3</td><td>▁▅▄▇▆▂▁█▄▇</td></tr><tr><td>128_valid_kappa/dataloader_idx_3</td><td>▁▃▄▅▆▂▂█▅▇</td></tr><tr><td>128_valid_optimistic_auc_pr/dataloader_idx_3</td><td>▁▃▄▄▄▃▇▇█▆</td></tr><tr><td>128_valid_optimistic_delta_auc_pr/dataloader_idx_3</td><td>▁▃▄▄▄▃▇▇█▆</td></tr><tr><td>128_valid_prec/dataloader_idx_3</td><td>▁▂▃▃▇▂▃█▅▆</td></tr><tr><td>128_valid_recall/dataloader_idx_3</td><td>▂▆▅█▅▃▁▆▃▆</td></tr><tr><td>128_valid_roc_auc/dataloader_idx_3</td><td>▁▅▂▅▂▁▄▇██</td></tr><tr><td>128_valid_size/dataloader_idx_3</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>16_valid_acc/dataloader_idx_0</td><td>▁▅█▄▃▄▅▄█▄</td></tr><tr><td>16_valid_avg_precision/dataloader_idx_0</td><td>▃▁▂▅▆▅▄▄██</td></tr><tr><td>16_valid_balanced_acc/dataloader_idx_0</td><td>▁▄▆▃▅▄▄▃█▃</td></tr><tr><td>16_valid_delta_auc_pr/dataloader_idx_0</td><td>▃▁▂▅▆▅▄▄██</td></tr><tr><td>16_valid_f1/dataloader_idx_0</td><td>▁▆▇▅▆▇▄▃█▂</td></tr><tr><td>16_valid_kappa/dataloader_idx_0</td><td>▁▄▆▃▄▄▄▃█▃</td></tr><tr><td>16_valid_optimistic_auc_pr/dataloader_idx_0</td><td>▄▁▂▆▆▅▄▄██</td></tr><tr><td>16_valid_optimistic_delta_auc_pr/dataloader_idx_0</td><td>▄▁▂▆▆▅▄▄██</td></tr><tr><td>16_valid_prec/dataloader_idx_0</td><td>▃▄█▂▁▁█▇▆▅</td></tr><tr><td>16_valid_recall/dataloader_idx_0</td><td>▃▆█▅▇█▂▂█▁</td></tr><tr><td>16_valid_roc_auc/dataloader_idx_0</td><td>▄▃▄▆█▇▆▁▄▆</td></tr><tr><td>16_valid_size/dataloader_idx_0</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>32_valid_acc/dataloader_idx_1</td><td>▃▁▄▆▇█▄▇▅▂</td></tr><tr><td>32_valid_avg_precision/dataloader_idx_1</td><td>▁▂▁▃█▆▇▄▆▇</td></tr><tr><td>32_valid_balanced_acc/dataloader_idx_1</td><td>▄▁▃▆██▅▆▅▂</td></tr><tr><td>32_valid_delta_auc_pr/dataloader_idx_1</td><td>▁▂▁▃█▆▇▄▆▇</td></tr><tr><td>32_valid_f1/dataloader_idx_1</td><td>▄▁▂▄█▇▃▆▄▁</td></tr><tr><td>32_valid_kappa/dataloader_idx_1</td><td>▄▁▄▆██▄▆▅▂</td></tr><tr><td>32_valid_optimistic_auc_pr/dataloader_idx_1</td><td>▁▂▁▃█▇█▄▆▆</td></tr><tr><td>32_valid_optimistic_delta_auc_pr/dataloader_idx_1</td><td>▁▂▁▃█▇█▄▆▆</td></tr><tr><td>32_valid_prec/dataloader_idx_1</td><td>▆▁▅▆▆█▄▇▆▃</td></tr><tr><td>32_valid_recall/dataloader_idx_1</td><td>▃▁▁▃█▅▃▅▄▁</td></tr><tr><td>32_valid_roc_auc/dataloader_idx_1</td><td>▁▃▃▃█▁▃▁▄▆</td></tr><tr><td>32_valid_size/dataloader_idx_1</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>64_valid_acc/dataloader_idx_2</td><td>█▄▆▄▃▄▁▅█▇</td></tr><tr><td>64_valid_avg_precision/dataloader_idx_2</td><td>▄▂▁▂▃▅▆▃▇█</td></tr><tr><td>64_valid_balanced_acc/dataloader_idx_2</td><td>█▄▆▃▃▄▁▅█▆</td></tr><tr><td>64_valid_delta_auc_pr/dataloader_idx_2</td><td>▄▂▁▂▃▅▆▃▇█</td></tr><tr><td>64_valid_f1/dataloader_idx_2</td><td>█▄▆▄▄▄▁▄▅▅</td></tr><tr><td>64_valid_kappa/dataloader_idx_2</td><td>█▄▆▄▃▄▁▅█▇</td></tr><tr><td>64_valid_optimistic_auc_pr/dataloader_idx_2</td><td>▄▃▁▂▃▅▆▄▇█</td></tr><tr><td>64_valid_optimistic_delta_auc_pr/dataloader_idx_2</td><td>▄▃▁▂▃▅▆▄▇█</td></tr><tr><td>64_valid_prec/dataloader_idx_2</td><td>▇▄▄▃▁▃▁▅█▆</td></tr><tr><td>64_valid_recall/dataloader_idx_2</td><td>█▅█▄▇▆▁▃▄▅</td></tr><tr><td>64_valid_roc_auc/dataloader_idx_2</td><td>▂▁▂▁▂▂▂▁▆█</td></tr><tr><td>64_valid_size/dataloader_idx_2</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>train_loss</td><td>▆▂▅▆▃▆▂▇▅▆▅▄▆▁▇▅▅▆▇▆▇█▆█▄▅▂▆▅▇▆▆▆▃▅▇▄▃▃▆</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>valid_loss/dataloader_idx_0</td><td>▂▁▄▄▄▅▅▆▅█</td></tr><tr><td>valid_loss/dataloader_idx_1</td><td>▅▁█▅▅▇▄▅▆█</td></tr><tr><td>valid_loss/dataloader_idx_2</td><td>▃▁▅▄█▇▅▄▁▃</td></tr><tr><td>valid_loss/dataloader_idx_3</td><td>█▄▅▄▆█▄▁▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>128_valid_acc/dataloader_idx_3</td><td>0.72999</td></tr><tr><td>128_valid_avg_precision/dataloader_idx_3</td><td>0.77862</td></tr><tr><td>128_valid_balanced_acc/dataloader_idx_3</td><td>0.7261</td></tr><tr><td>128_valid_delta_auc_pr/dataloader_idx_3</td><td>0.31169</td></tr><tr><td>128_valid_f1/dataloader_idx_3</td><td>0.70542</td></tr><tr><td>128_valid_kappa/dataloader_idx_3</td><td>0.45032</td></tr><tr><td>128_valid_optimistic_auc_pr/dataloader_idx_3</td><td>0.76971</td></tr><tr><td>128_valid_optimistic_delta_auc_pr/dataloader_idx_3</td><td>0.30277</td></tr><tr><td>128_valid_prec/dataloader_idx_3</td><td>0.70049</td></tr><tr><td>128_valid_recall/dataloader_idx_3</td><td>0.71881</td></tr><tr><td>128_valid_roc_auc/dataloader_idx_3</td><td>0.79395</td></tr><tr><td>128_valid_size/dataloader_idx_3</td><td>69.32609</td></tr><tr><td>16_valid_acc/dataloader_idx_0</td><td>0.67791</td></tr><tr><td>16_valid_avg_precision/dataloader_idx_0</td><td>0.72715</td></tr><tr><td>16_valid_balanced_acc/dataloader_idx_0</td><td>0.66814</td></tr><tr><td>16_valid_delta_auc_pr/dataloader_idx_0</td><td>0.2594</td></tr><tr><td>16_valid_f1/dataloader_idx_0</td><td>0.62902</td></tr><tr><td>16_valid_kappa/dataloader_idx_0</td><td>0.33829</td></tr><tr><td>16_valid_optimistic_auc_pr/dataloader_idx_0</td><td>0.72029</td></tr><tr><td>16_valid_optimistic_delta_auc_pr/dataloader_idx_0</td><td>0.25254</td></tr><tr><td>16_valid_prec/dataloader_idx_0</td><td>0.67957</td></tr><tr><td>16_valid_recall/dataloader_idx_0</td><td>0.61658</td></tr><tr><td>16_valid_roc_auc/dataloader_idx_0</td><td>0.73516</td></tr><tr><td>16_valid_size/dataloader_idx_0</td><td>141.0</td></tr><tr><td>32_valid_acc/dataloader_idx_1</td><td>0.69434</td></tr><tr><td>32_valid_avg_precision/dataloader_idx_1</td><td>0.73972</td></tr><tr><td>32_valid_balanced_acc/dataloader_idx_1</td><td>0.68131</td></tr><tr><td>32_valid_delta_auc_pr/dataloader_idx_1</td><td>0.27019</td></tr><tr><td>32_valid_f1/dataloader_idx_1</td><td>0.64172</td></tr><tr><td>32_valid_kappa/dataloader_idx_1</td><td>0.36602</td></tr><tr><td>32_valid_optimistic_auc_pr/dataloader_idx_1</td><td>0.73298</td></tr><tr><td>32_valid_optimistic_delta_auc_pr/dataloader_idx_1</td><td>0.26345</td></tr><tr><td>32_valid_prec/dataloader_idx_1</td><td>0.6823</td></tr><tr><td>32_valid_recall/dataloader_idx_1</td><td>0.63153</td></tr><tr><td>32_valid_roc_auc/dataloader_idx_1</td><td>0.76146</td></tr><tr><td>32_valid_size/dataloader_idx_1</td><td>130.93478</td></tr><tr><td>64_valid_acc/dataloader_idx_2</td><td>0.71452</td></tr><tr><td>64_valid_avg_precision/dataloader_idx_2</td><td>0.76191</td></tr><tr><td>64_valid_balanced_acc/dataloader_idx_2</td><td>0.70945</td></tr><tr><td>64_valid_delta_auc_pr/dataloader_idx_2</td><td>0.29516</td></tr><tr><td>64_valid_f1/dataloader_idx_2</td><td>0.68071</td></tr><tr><td>64_valid_kappa/dataloader_idx_2</td><td>0.41675</td></tr><tr><td>64_valid_optimistic_auc_pr/dataloader_idx_2</td><td>0.75711</td></tr><tr><td>64_valid_optimistic_delta_auc_pr/dataloader_idx_2</td><td>0.29036</td></tr><tr><td>64_valid_prec/dataloader_idx_2</td><td>0.6904</td></tr><tr><td>64_valid_recall/dataloader_idx_2</td><td>0.68376</td></tr><tr><td>64_valid_roc_auc/dataloader_idx_2</td><td>0.77971</td></tr><tr><td>64_valid_size/dataloader_idx_2</td><td>110.76087</td></tr><tr><td>epoch</td><td>9</td></tr><tr><td>train_loss</td><td>0.27736</td></tr><tr><td>trainer/global_step</td><td>12889</td></tr><tr><td>valid_loss/dataloader_idx_0</td><td>0.66868</td></tr><tr><td>valid_loss/dataloader_idx_1</td><td>0.60632</td></tr><tr><td>valid_loss/dataloader_idx_2</td><td>0.55571</td></tr><tr><td>valid_loss/dataloader_idx_3</td><td>0.54642</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dauntless-dew-615</strong> at: <a href='https://wandb.ai/dest/MXM/runs/8yr5woq6' target=\"_blank\">https://wandb.ai/dest/MXM/runs/8yr5woq6</a><br/>Synced 3 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230527_201748-8yr5woq6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:8yr5woq6). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad93c7efbe0c449c8fd6ff2345547c50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.0166682136166249, max=1.0))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/FS-MOL/wandb/run-20230527_220125-c5pdvvmy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dest/MXM-Test/runs/c5pdvvmy' target=\"_blank\">light-night-19</a></strong> to <a href='https://wandb.ai/dest/MXM-Test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dest/MXM-Test' target=\"_blank\">https://wandb.ai/dest/MXM-Test</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dest/MXM-Test/runs/c5pdvvmy' target=\"_blank\">https://wandb.ai/dest/MXM-Test/runs/c5pdvvmy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/loggers/wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ceccb8478c94f7da6babcb1726bc1af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('16_test_size', ...)` in your `test_step.0` but the value needs to be floating point. Converting it to torch.float32.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('32_test_size', ...)` in your `test_step.1` but the value needs to be floating point. Converting it to torch.float32.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('64_test_size', ...)` in your `test_step.2` but the value needs to be floating point. Converting it to torch.float32.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('128_test_size', ...)` in your `test_step.3` but the value needs to be floating point. Converting it to torch.float32.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "          Test metric                     DataLoader 0                    DataLoader 1\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "          128_test_acc\n",
      "     128_test_avg_precision\n",
      "     128_test_balanced_acc\n",
      "     128_test_delta_auc_pr\n",
      "          128_test_f1\n",
      "         128_test_kappa\n",
      "   128_test_optimistic_auc_pr\n",
      "128_test_optimistic_delta_auc_pr\n",
      "         128_test_prec\n",
      "        128_test_recall\n",
      "        128_test_roc_auc\n",
      "         128_test_size\n",
      "          16_test_acc                  0.6561748459663684\n",
      "     16_test_avg_precision             0.6735776124394305\n",
      "      16_test_balanced_acc             0.6466572828182953\n",
      "      16_test_delta_auc_pr            0.21322405746593268\n",
      "           16_test_f1                  0.6073572579598342\n",
      "         16_test_kappa                0.29076901390448034\n",
      "   16_test_optimistic_auc_pr           0.6622020678695449\n",
      "16_test_optimistic_delta_auc_pr        0.2018485128960469\n",
      "          16_test_prec                 0.6249127506188803\n",
      "         16_test_recall                0.6230091969376178\n",
      "        16_test_roc_auc                0.6998678811569286\n",
      "          16_test_size                 139.5054931640625\n",
      "          32_test_acc                                                  0.6795617756842951\n",
      "     32_test_avg_precision                                             0.7127826281176937\n",
      "      32_test_balanced_acc                                             0.6749557704051329\n",
      "      32_test_delta_auc_pr                                             0.2449270257579327\n",
      "           32_test_f1                                                  0.6500878400796525\n",
      "         32_test_kappa                                                0.35006090182592664\n",
      "   32_test_optimistic_auc_pr                                           0.705922227247968\n",
      "32_test_optimistic_delta_auc_pr                                       0.23806662488820707\n",
      "          32_test_prec                                                 0.6568443963820673\n",
      "         32_test_recall                                                0.6566861280210105\n",
      "        32_test_roc_auc                                                0.7350875844013673\n",
      "          32_test_size                                                 129.6593475341797\n",
      "          64_test_acc\n",
      "     64_test_avg_precision\n",
      "      64_test_balanced_acc\n",
      "      64_test_delta_auc_pr\n",
      "           64_test_f1\n",
      "         64_test_kappa\n",
      "   64_test_optimistic_auc_pr\n",
      "64_test_optimistic_delta_auc_pr\n",
      "          64_test_prec\n",
      "         64_test_recall\n",
      "        64_test_roc_auc\n",
      "          64_test_size\n",
      "           valid_loss                  0.7232455015182495              0.6394444108009338\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "          Test metric                     DataLoader 2                    DataLoader 3\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "          128_test_acc                                                 0.7117209996582813\n",
      "     128_test_avg_precision                                            0.7568841614991545\n",
      "     128_test_balanced_acc                                             0.7099221031679743\n",
      "     128_test_delta_auc_pr                                            0.29412000540138755\n",
      "          128_test_f1                                                  0.6871981698332644\n",
      "         128_test_kappa                                                0.4171324602463912\n",
      "   128_test_optimistic_auc_pr                                          0.7472878360246921\n",
      "128_test_optimistic_delta_auc_pr                                       0.2845236799269254\n",
      "         128_test_prec                                                 0.6859235503786401\n",
      "        128_test_recall                                                0.7039049973867184\n",
      "        128_test_roc_auc                                               0.7692907090286832\n",
      "         128_test_size                                                 69.13735961914062\n",
      "          16_test_acc\n",
      "     16_test_avg_precision\n",
      "      16_test_balanced_acc\n",
      "      16_test_delta_auc_pr\n",
      "           16_test_f1\n",
      "         16_test_kappa\n",
      "   16_test_optimistic_auc_pr\n",
      "16_test_optimistic_delta_auc_pr\n",
      "          16_test_prec\n",
      "         16_test_recall\n",
      "        16_test_roc_auc\n",
      "          16_test_size\n",
      "          32_test_acc\n",
      "     32_test_avg_precision\n",
      "      32_test_balanced_acc\n",
      "      32_test_delta_auc_pr\n",
      "           32_test_f1\n",
      "         32_test_kappa\n",
      "   32_test_optimistic_auc_pr\n",
      "32_test_optimistic_delta_auc_pr\n",
      "          32_test_prec\n",
      "         32_test_recall\n",
      "        32_test_roc_auc\n",
      "          32_test_size\n",
      "          64_test_acc                  0.6992496472253982\n",
      "     64_test_avg_precision             0.7310116144403032\n",
      "      64_test_balanced_acc             0.6960156135610768\n",
      "      64_test_delta_auc_pr             0.2650941284191099\n",
      "           64_test_f1                  0.6730454277556628\n",
      "         64_test_kappa                 0.3904589570816984\n",
      "   64_test_optimistic_auc_pr           0.7251596650592719\n",
      "64_test_optimistic_delta_auc_pr       0.25924217903807845\n",
      "          64_test_prec                 0.6721423058691177\n",
      "         64_test_recall                0.6826095340385632\n",
      "        64_test_roc_auc                0.7542351417338095\n",
      "          64_test_size                 109.79669952392578\n",
      "           valid_loss                  0.5930635929107666              0.5685642957687378\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "from typing import Any, Callable, Optional, Union\n",
    "from pytorch_lightning.core.optimizer import LightningOptimizer\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from torch_geometric.nn.models.autoencoder import VGAE\n",
    "from MXMNet.model import MXMNet, Config\n",
    "from fs_mol.models.protonet import PyG_GraphFeatureExtractor, GraphFeatureExtractor\n",
    "from fs_mol.modules.graph_feature_extractor import GraphFeatureExtractorConfig\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from torch_geometric.data import Batch\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from fs_mol.transformer_based_pretraining import ScaledDotProductAttention\n",
    "from fs_mol.utils.metrics import compute_binary_task_metrics\n",
    "import numpy as np\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "class ClipLike(pl.LightningModule):\n",
    "    def __init__(self, config: TrainConfig):\n",
    "        self.config = config\n",
    "        super().__init__()\n",
    "        self.automatic_optimization = False\n",
    "        self.graph_encoder = MXMNet(\n",
    "            Config(config.dim, config.layer, config.cutoff, config.encoder_dims, 512),\n",
    "            num_spherical=config.num_spherical,\n",
    "            num_radial=config.num_radial,\n",
    "            envelope_exponent=config.envelope_exponent,\n",
    "            dropout=config.dropout,\n",
    "        )\n",
    "\n",
    "    def calculate_feats(self, batch):\n",
    "        encoded_graphs = self.graph_encoder(batch)\n",
    "        # feats = torch.cat([encoded_graphs, batch.fingerprint.reshape(-1, 2048)], dim=1)\n",
    "\n",
    "        return encoded_graphs\n",
    "\n",
    "    def calc_loss(self, input):\n",
    "        batch, labels, index_map = input\n",
    "        feats = self.graph_encoder(batch)\n",
    "        feats = F.normalize(feats, dim=-1)\n",
    "\n",
    "        support_feats = feats[index_map == 0]\n",
    "        query_feats = feats[index_map == 1]\n",
    "\n",
    "        support_labels = labels[index_map == 0]\n",
    "        query_labels = labels[index_map == 1]\n",
    "\n",
    "        logits = calculate_mahalanobis_logits(\n",
    "            support_feats, support_labels, query_feats, torch.device(\"cuda\")\n",
    "        )\n",
    "        loss = F.cross_entropy(logits / config.temprature, query_labels)\n",
    "\n",
    "        return loss, logits, query_labels\n",
    "\n",
    "    def on_train_end(self):\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    def training_step(self, inputs, batch_idx):\n",
    "        opt = self.optimizers()\n",
    "        loss_acc = 0\n",
    "        count = 1\n",
    "        for input in inputs:\n",
    "            loss, _, _ = self.calc_loss(input)\n",
    "            self.manual_backward(loss)\n",
    "            loss_acc += loss\n",
    "            count += 1\n",
    "\n",
    "        self.log(\"train_loss\", loss_acc / count, on_step=True, batch_size=config.batch_size)\n",
    "        if config.accumulate_grad_batches <= 1:\n",
    "            # self.clip_gradients(opt, gradient_clip_val=0.5, gradient_clip_algorithm=\"norm\")\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "        else:\n",
    "            if (batch_idx + 1) % config.accumulate_grad_batches == 0:\n",
    "                # self.clip_gradients(opt, gradient_clip_val=0.5, gradient_clip_algorithm=\"norm\")\n",
    "                opt.step()\n",
    "                opt.zero_grad()\n",
    "\n",
    "        # return loss\n",
    "\n",
    "        # print('Going to Concat:')\n",
    "        # flatted_vectors = torch.cat(resulting_vectors, dim=0)\n",
    "        # support_feats = flatted_vectors[index_map == 0]\n",
    "        # query_feats = flatted_vectors[index_map == 1]\n",
    "\n",
    "        # print('Done!')\n",
    "        # return loss\n",
    "        # for batch in batches:\n",
    "        #     loss, _, _ = self.calc_loss(batch)\n",
    "\n",
    "        #     self.manual_backward(loss)\n",
    "        #     self.log('train_loss', loss, on_step=True, on_epoch=False, batch_size=config.batch_size)\n",
    "\n",
    "        # if (batch_idx + 1) % self.config.accumulate_grad_batches == 0:\n",
    "        #     self.clip_gradients(opt, gradient_clip_val=.5, gradient_clip_algorithm=\"norm\")\n",
    "        #     opt.step()\n",
    "\n",
    "    def validation_step(self, batches, batch_idx, loader_idx, dataloader_idx=0):\n",
    "        for batch in batches:\n",
    "            valid_loss, logits, query_labels = self.calc_loss(batch)\n",
    "\n",
    "            self.log(\"valid_loss\", valid_loss, on_step=False, on_epoch=True, batch_size=256)\n",
    "\n",
    "            batch_preds = F.softmax(logits, dim=1).detach().cpu().numpy()\n",
    "\n",
    "            metrics = compute_binary_task_metrics(\n",
    "                predictions=batch_preds[:, 1], labels=query_labels.detach().cpu().numpy()\n",
    "            )\n",
    "\n",
    "            for k, v in metrics.__dict__.items():\n",
    "                self.log(\n",
    "                    f\"{2 ** (loader_idx + 4)}_valid_{k}\",\n",
    "                    v,\n",
    "                    on_epoch=True,\n",
    "                    on_step=False,\n",
    "                    batch_size=config.valid_batch_size,\n",
    "                )\n",
    "\n",
    "    def test_step(self, batches, batch_idx, loader_idx, dataloader_idx=0):\n",
    "        for batch in batches:\n",
    "            valid_loss, logits, query_labels = self.calc_loss(batch)\n",
    "\n",
    "            self.log(\"valid_loss\", valid_loss, on_step=False, on_epoch=True, batch_size=256)\n",
    "\n",
    "            batch_preds = F.softmax(logits, dim=1).detach().cpu().numpy()\n",
    "\n",
    "            metrics = compute_binary_task_metrics(\n",
    "                predictions=batch_preds[:, 1], labels=query_labels.detach().cpu().numpy()\n",
    "            )\n",
    "\n",
    "            for k, v in metrics.__dict__.items():\n",
    "                self.log(\n",
    "                    f\"{2 ** (loader_idx + 4)}_test_{k}\",\n",
    "                    v,\n",
    "                    on_epoch=True,\n",
    "                    on_step=False,\n",
    "                    batch_size=config.valid_batch_size,\n",
    "                )\n",
    "\n",
    "    def optimizer_step(\n",
    "        self,\n",
    "        epoch: int,\n",
    "        batch_idx: int,\n",
    "        optimizer: Optimizer,\n",
    "        optimizer_closure: Callable[[], Any] = None,\n",
    "    ) -> None:\n",
    "        return super().optimizer_step(epoch, batch_idx, optimizer, optimizer_closure)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(\n",
    "            self.parameters(),\n",
    "            lr=self.config.learning_rate,\n",
    "            weight_decay=config.weight_decay,\n",
    "            fused=True,\n",
    "        )\n",
    "\n",
    "\n",
    "# wandb.init(project='MXM-Test', config=config)\n",
    "# trainer = pl.Trainer(accelerator='gpu', devices=1, max_epochs=10, log_every_n_steps=1, logger=WandbLogger(), default_root_dir='/FS-MOL/MXM_Checkpoint/')\n",
    "# model = ClipLike(config).load_from_checkpoint('/FS-MOL/lightning_logs/m38aeocm/checkpoints/epoch=19-step=6440.ckpt', config=config)\n",
    "\n",
    "# trainer.test(model, dataloaders=[test_dl_16, test_dl_32, test_dl_64, test_dl_128])\n",
    "path = \"/FS-MOL/lightning_logs/8yr5woq6/checkpoints/epoch=9-step=3220-v1.ckpt\"\n",
    "\n",
    "# path = None\n",
    "\n",
    "\n",
    "def train_mxm(config):\n",
    "    if path is not None:\n",
    "        run_id = path.split(\"/\")[3]\n",
    "        wandb.init(project=\"MXM\", config=config, id=run_id, resume=True)\n",
    "        model = ClipLike.load_from_checkpoint(path, config=config)\n",
    "    else:\n",
    "        wandb.init(project=\"MXM\", config=config)\n",
    "        model = ClipLike(config)\n",
    "    wandb.watch(model, log=\"all\")\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator=\"gpu\",\n",
    "        devices=1,\n",
    "        max_epochs=10,\n",
    "        log_every_n_steps=1,\n",
    "        logger=WandbLogger(),\n",
    "        default_root_dir=\"/FS-MOL/MXM_Checkpoint/\",\n",
    "    )\n",
    "\n",
    "    trainer.fit(\n",
    "        model=model,\n",
    "        train_dataloaders=train_dl,\n",
    "        val_dataloaders=[valid_dl_16, valid_dl_32, valid_dl_64, valid_dl_128],\n",
    "    )\n",
    "\n",
    "\n",
    "def test_mxm(path):\n",
    "    wandb.init(project=\"MXM-Test\", config=config)\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator=\"gpu\",\n",
    "        devices=1,\n",
    "        max_epochs=20,\n",
    "        log_every_n_steps=1,\n",
    "        logger=WandbLogger(),\n",
    "        default_root_dir=\"/FS-MOL/MXM_Checkpoint/\",\n",
    "    )\n",
    "    model = ClipLike.load_from_checkpoint(path, config=config)\n",
    "\n",
    "    trainer.test(model, dataloaders=[test_dl_16, test_dl_32, test_dl_64, test_dl_128])\n",
    "\n",
    "\n",
    "test_mxm(path)\n",
    "train_mxm(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
