{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "FS_MOL_CHECKOUT_PATH = os.path.abspath(\"../\")\n",
    "\n",
    "os.chdir(FS_MOL_CHECKOUT_PATH)\n",
    "sys.path.insert(0, FS_MOL_CHECKOUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from fs_mol.data_modules.MXM_datamodule import MXMDataModule, MXMDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class MXMNetTrainingConfig:\n",
    "    # Training Settings:\n",
    "    batch_size: int = 8\n",
    "    train_support_count: int = 16\n",
    "    train_query_count: int = 16\n",
    "    train_shuffle: bool = True\n",
    "\n",
    "    temprature: float = 0.07\n",
    "\n",
    "    # Validation Settings:\n",
    "    valid_support_count: int = 64\n",
    "    valid_batch_size: int = 256\n",
    "\n",
    "    # Model Settings:\n",
    "    envelope_exponent: int = 6\n",
    "    num_spherical: int = 7\n",
    "    num_radial: int = 5\n",
    "    dim: int = 128\n",
    "    cutoff: int = 3.0\n",
    "    layer: int = 5\n",
    "\n",
    "    accumulate_grad_batches: int = 4\n",
    "    learning_rate: float = 1e-4\n",
    "    weight_decay: float = 1e-5\n",
    "\n",
    "    dropout: float = 0.2\n",
    "\n",
    "    encoder_dims = [128, 128, 256, 256, 512, 512]\n",
    "\n",
    "\n",
    "config = MXMNetTrainingConfig()\n",
    "\n",
    "data_module = MXMDataModule(\n",
    "    \"/FS-MOL/data/mxm/\",\n",
    "    batch_size=config.batch_size,\n",
    "    support_size=config.train_support_count,\n",
    "    query_size=config.train_query_count,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:ls1b4s7r) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e8d163794db4d93b6aecb87caee9cd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.010 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.440296…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">playful-lion-35</strong> at: <a href='https://wandb.ai/dest/MXMNet_New/runs/ls1b4s7r' target=\"_blank\">https://wandb.ai/dest/MXMNet_New/runs/ls1b4s7r</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230716_104948-ls1b4s7r/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:ls1b4s7r). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8944e2aba3174fa4a64510268be8720f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016668199933337745, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/FS-MOL/wandb/run-20230716_105018-ls1b4s7r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dest/MXMNet_New/runs/ls1b4s7r' target=\"_blank\">playful-lion-35</a></strong> to <a href='https://wandb.ai/dest/MXMNet_New' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dest/MXMNet_New' target=\"_blank\">https://wandb.ai/dest/MXMNet_New</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dest/MXMNet_New/runs/ls1b4s7r' target=\"_blank\">https://wandb.ai/dest/MXMNet_New/runs/ls1b4s7r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/loggers/wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached result from /FS-MOL/data/mxm/train/../cached/task_name_length_3aff30c7f2cc0298d1f5e520da7b4b1b8dbf22f66c4a76252e433c0bca2d8655.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory ./lightning_logs/ls1b4s7r/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type   | Params\n",
      "-----------------------------------------\n",
      "0 | graph_encoder | MXMNet | 1.1 M \n",
      "-----------------------------------------\n",
      "1.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 M     Total params\n",
      "4.384     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4059a5d248614cc9bfa0d10c8c52e375",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7b6324013134d1d896425db6c03a051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b56b3bc96a1e4757bbfb1474bb064e21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ffd2c35155349ac9c8725229daed85e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efe00be0b998442ab8a0829d1d2f463b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from typing import Any, Optional\n",
    "import numpy as np\n",
    "from pytorch_lightning.utilities.types import STEP_OUTPUT\n",
    "from sklearn.metrics import auc, precision_recall_curve\n",
    "import wandb\n",
    "from MHNfs.mhnfs.modules import CrossAttentionModule_2\n",
    "from MXMNet.model import Config, MXMNet\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from fewshot_utils.is_debugger_attached import is_debugger_attached\n",
    "\n",
    "from fs_mol.models.protonet import calculate_mahalanobis_logits\n",
    "\n",
    "\n",
    "class MXMNetLighteningModule(pl.LightningModule):\n",
    "    def __init__(self, config: MXMNetTrainingConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.graph_encoder = MXMNet(\n",
    "            Config(config.dim, config.layer, config.cutoff, config.encoder_dims, 512)\n",
    "        )\n",
    "\n",
    "        self.cross_attn = CrossAttentionModule_2(config.dim * config.layer, 64, 8, 0.5)\n",
    "\n",
    "        self.validation_step_output = []\n",
    "\n",
    "    def get_support_query(self, input_tensor, is_query_index):\n",
    "        support_indices = (is_query_index == 0).nonzero().squeeze(1)\n",
    "        query_indices = (is_query_index == 1).nonzero().squeeze(1)\n",
    "\n",
    "        return input_tensor[support_indices], input_tensor[query_indices]\n",
    "\n",
    "    def select_batch(self, input_tensor, batch_index, batch_no):\n",
    "        current_batch_indices = (batch_index == batch_no).nonzero().squeeze(1)\n",
    "\n",
    "        return input_tensor[current_batch_indices]\n",
    "\n",
    "    # def get_logits_with_attn(self, graph_reprs, labels, query_index, batch_index):\n",
    "    #     all_logits = []\n",
    "    #     batch_size = batch_index.max().item() + 1\n",
    "    #     for i in range(batch_size):\n",
    "    #         current_batch_graph_repr = self.select_batch(graph_reprs, batch_index, i)\n",
    "    #         current_batch_labels = self.select_batch(labels, batch_index, i)\n",
    "    #         current_batch_query_index = self.select_batch(query_index, batch_index, i)\n",
    "\n",
    "    #         support_repr, query_repr = self.get_support_query(\n",
    "    #             current_batch_graph_repr, current_batch_query_index\n",
    "    #         )\n",
    "\n",
    "    #         support_labels, query_labels = self.get_support_query(\n",
    "    #             current_batch_labels, current_batch_query_index\n",
    "    #         )\n",
    "\n",
    "    #         support_negative_indices = (support_labels == 0).nonzero().squeeze(1)\n",
    "    #         support_positive_indices = (support_labels == 1).nonzero().squeeze(1)\n",
    "\n",
    "    #         support_positive = support_repr[support_positive_indices]\n",
    "    #         support_negative = support_repr[support_negative_indices]\n",
    "\n",
    "    #         query_attn, support_active_attn, support_inactive_attn = self.cross_attn(\n",
    "    #             query_repr, support_positive, support_negative\n",
    "    #         )\n",
    "\n",
    "    #     return torch.cat(all_logits, dim=0)\n",
    "\n",
    "    def get_logits_per_batch(self, graph_reprs, labels, query_index, batch_index):\n",
    "        all_logits = []\n",
    "        batch_size = batch_index.max().item() + 1\n",
    "        for i in range(batch_size):\n",
    "            current_batch_graph_repr = self.select_batch(graph_reprs, batch_index, i)\n",
    "            current_batch_labels = self.select_batch(labels, batch_index, i)\n",
    "\n",
    "            current_batch_query_index = self.select_batch(query_index, batch_index, i)\n",
    "\n",
    "            support_repr, query_repr = self.get_support_query(\n",
    "                current_batch_graph_repr, current_batch_query_index\n",
    "            )\n",
    "            support_labels, query_labels = self.get_support_query(\n",
    "                current_batch_labels, current_batch_query_index\n",
    "            )\n",
    "\n",
    "            logits = calculate_mahalanobis_logits(\n",
    "                support_repr, support_labels, query_repr, device=self.device\n",
    "            )\n",
    "\n",
    "            all_logits.append(logits)\n",
    "        return torch.cat(all_logits, dim=0)\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        input_graphs, is_query, labels, batch_index = (\n",
    "            batch[\"graphs\"],\n",
    "            batch[\"is_query\"],\n",
    "            batch[\"labels\"],\n",
    "            batch[\"batch_index\"],\n",
    "        )\n",
    "        graph_representations = self.graph_encoder(input_graphs)\n",
    "        # graph_representations = F.normalize(graph_representations, dim=-1)\n",
    "        _, query_labels = self.get_support_query(labels, is_query)\n",
    "\n",
    "        logits = self.get_logits_per_batch(graph_representations, labels, is_query, batch_index)\n",
    "\n",
    "        loss = F.cross_entropy(logits / self.config.temprature, query_labels)\n",
    "\n",
    "        self.log(\"loss\", loss, on_step=True, on_epoch=False, batch_size=self.config.batch_size)\n",
    "        self.log(\n",
    "            \"loss_per_epoch\", loss, on_epoch=True, on_step=False, batch_size=self.config.batch_size\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_graphs, is_query, labels, batch_index = (\n",
    "            batch[\"graphs\"],\n",
    "            batch[\"is_query\"],\n",
    "            batch[\"labels\"],\n",
    "            batch[\"batch_index\"],\n",
    "        )\n",
    "\n",
    "        graph_representations = self.graph_encoder(input_graphs)\n",
    "        # graph_representations = F.normalize(graph_representations, dim=-1)\n",
    "\n",
    "        logits = self.get_logits_per_batch(graph_representations, labels, is_query, batch_index)\n",
    "        _, query_labels = self.get_support_query(labels, is_query)\n",
    "\n",
    "        auc_pr = self.calculate_delta_auc_pr(logits, query_labels)\n",
    "\n",
    "        self.validation_step_output.append(auc_pr)\n",
    "\n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        mean_delta_auc_pr = np.mean(self.validation_step_output)\n",
    "        self.log(\"mean_delta_auc_pr\", mean_delta_auc_pr)\n",
    "\n",
    "        self.validation_step_output.clear()\n",
    "\n",
    "    def calculate_delta_auc_pr(self, logits, targets):\n",
    "        predictions = F.softmax(logits, dim=1)[:, 1]\n",
    "\n",
    "        precision, recall, _ = precision_recall_curve(\n",
    "            targets.detach().cpu().numpy(), predictions.detach().cpu().numpy()\n",
    "        )\n",
    "\n",
    "        auc_score = auc(recall, precision)\n",
    "\n",
    "        random_classifier_auc_pr = np.mean(targets.detach().cpu().numpy())\n",
    "\n",
    "        return auc_score - random_classifier_auc_pr\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(\n",
    "            self.parameters(),\n",
    "            lr=self.config.learning_rate,\n",
    "            weight_decay=config.weight_decay,\n",
    "            fused=True,\n",
    "        )\n",
    "\n",
    "\n",
    "sample_path = \"/FS-MOL/lightning_logs/ls1b4s7r/checkpoints/epoch=19-step=54920.ckpt\"\n",
    "\n",
    "wandb_enabled = not is_debugger_attached()\n",
    "\n",
    "\n",
    "def train(path):\n",
    "    run_id = path.split(\"/\")[3] if path is not None else None\n",
    "    if wandb_enabled:\n",
    "        wandb.init(project=\"MXMNet_New\", config=config, id=run_id if run_id is not None else None)\n",
    "    model = (\n",
    "        MXMNetLighteningModule.load_from_checkpoint(path, config=config)\n",
    "        if path is not None\n",
    "        else MXMNetLighteningModule(config)\n",
    "    )\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator=\"gpu\",\n",
    "        devices=1,\n",
    "        max_epochs=20,\n",
    "        log_every_n_steps=1,\n",
    "        logger=WandbLogger() if wandb_enabled else None,\n",
    "        default_root_dir=\"/FS-MOL/MXM_Checkpoint/\",\n",
    "    )\n",
    "    if wandb_enabled:\n",
    "        wandb.watch(model, log=\"all\")\n",
    "    trainer.fit(model, datamodule=data_module)\n",
    "\n",
    "\n",
    "train(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
