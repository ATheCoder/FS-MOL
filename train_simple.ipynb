{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'FS-Mol'\n",
      "/notebooks\n"
     ]
    }
   ],
   "source": [
    "%cd FS-Mol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/athecoder/FS-Mol/env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from dpu_utils.utils import run_and_debug, RichPath\n",
    "from fs_mol.data import FSMolDataset, FSMolTaskSample, DataFold\n",
    "from fs_mol.models.protonet import PrototypicalNetwork\n",
    "from fs_mol.custom.data import generate_episodic_iterable\n",
    "from fs_mol.custom.model import SimpleGraphConv\n",
    "\n",
    "\n",
    "from fs_mol.data.protonet import (\n",
    "    ProtoNetBatch,\n",
    "    get_protonet_task_sample_iterable,\n",
    "    get_protonet_batcher,\n",
    "    task_sample_to_pn_task_sample,\n",
    ")\n",
    "\n",
    "fsmol_dataset = FSMolDataset.from_directory(\n",
    "        directory=RichPath.create('datasets/fs-mol/'),\n",
    "        task_list_file=RichPath.create('datasets/fsmol-0.1.json'), num_workers=0)\n",
    "\n",
    "# Adding a `,` will cause the variable to become a singke tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, importlib\n",
    "\n",
    "import torch\n",
    "from fs_mol.custom.data import generate_episodic_iterable\n",
    "from fs_mol.custom.model import SimpleGraphConv\n",
    "\n",
    "train_iterable = generate_episodic_iterable(fsmol_dataset, DataFold.TRAIN)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "model = SimpleGraphConv().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.74699592590332\n",
      "4.848016917705536\n",
      "5.060467839241028\n",
      "4.060458034276962\n",
      "3.4685513496398928\n",
      "7.464049418767293\n",
      "6.685004472732544\n",
      "6.482004135847092\n",
      "5.8517959581481085\n",
      "5.336916887760163\n",
      "5.227851661768827\n",
      "5.893453409274419\n",
      "6.52497164102701\n",
      "6.2178031631878445\n",
      "5.978040480613709\n",
      "5.703562639653683\n",
      "5.549973298521603\n",
      "5.339247763156891\n",
      "5.223535368317052\n",
      "5.01622816324234\n",
      "4.875886417570568\n",
      "4.685071622783488\n",
      "4.871647982493691\n",
      "4.716726002593835\n",
      "4.582151081562042\n",
      "4.480991874749844\n",
      "4.426340171584377\n",
      "4.302618037377085\n",
      "4.234833735844185\n",
      "4.154806254307429\n",
      "4.055301195190799\n",
      "4.034584729000926\n",
      "3.9352473165049697\n",
      "3.860147882910336\n",
      "3.763261011668614\n",
      "3.7597418361239963\n",
      "3.7183488317438074\n",
      "3.653952846401616\n",
      "3.608600527812273\n",
      "3.5374699532985687\n",
      "3.4722902018849444\n",
      "3.4032038705689565\n",
      "3.3506866859835247\n",
      "3.3677037331190975\n",
      "3.325014583269755\n",
      "3.2721408955428912\n",
      "3.2466160173111773\n",
      "3.1933562755584717\n",
      "3.1432684015254586\n",
      "3.095721527338028\n",
      "3.0699433988215876\n",
      "3.086291853051919\n",
      "3.0413801456397436\n",
      "2.997219896978802\n",
      "3.0842646154490385\n",
      "3.049687886876719\n",
      "3.2109009832666633\n",
      "3.192728987027859\n",
      "3.2753957398867204\n",
      "3.361781397461891\n",
      "3.438492500391163\n",
      "3.4278482227556166\n",
      "3.3839292119419766\n",
      "3.3477331390604377\n",
      "3.3221670985221863\n",
      "3.413633298693281\n",
      "3.382222647987195\n",
      "3.4030800733496163\n",
      "3.3787914333136184\n",
      "3.354883106265749\n",
      "3.396124095144406\n",
      "3.391127213007874\n",
      "3.364579487336825\n",
      "3.339963818724091\n",
      "3.3070180002848306\n",
      "3.2831505411549617\n",
      "3.2577470479073463\n",
      "3.225003109528468\n",
      "3.205433830430236\n",
      "3.202833926677704\n",
      "3.2356795852566944\n",
      "3.2147374691032784\n",
      "3.1942044496536255\n",
      "3.182391752799352\n",
      "3.1693395488402425\n",
      "3.1420803818591807\n",
      "3.1142919269101372\n",
      "3.0874424705451187\n",
      "3.061419705996353\n",
      "3.042096538676156\n",
      "3.0394515028366675\n",
      "3.0968999739574348\n",
      "3.075680616722312\n",
      "3.050878886212694\n",
      "3.026203357545953\n",
      "3.0029891406496367\n",
      "2.9822310442777025\n",
      "2.9832177405454674\n",
      "2.9719228804713547\n",
      "2.949448752999306\n",
      "3.0289661536122314\n",
      "3.0128868870875416\n",
      "3.0106578295670667\n",
      "2.9890335104786434\n",
      "2.967051460629418\n",
      "2.9456206768188835\n",
      "2.9455406069755554\n",
      "2.9464494900570974\n",
      "2.9308318419194004\n",
      "2.9602138979868458\n",
      "2.957988176796887\n",
      "2.941029549177204\n",
      "2.921511719712114\n",
      "2.9036545136518646\n",
      "2.886617195606232\n",
      "2.8690794007531526\n",
      "2.854114171786186\n",
      "2.865321411924847\n",
      "2.8505905015128\n",
      "2.848070740699768\n",
      "2.911769701429635\n",
      "2.8997725375363084\n",
      "2.8817339602524674\n",
      "2.864140382697505\n",
      "2.846511971473694\n",
      "2.842159476545122\n",
      "2.8913002624286444\n",
      "2.9017234789207578\n",
      "2.938886562059092\n",
      "2.943829726255857\n",
      "2.9268105503256994\n",
      "2.9124018495733086\n",
      "2.9234599708614493\n",
      "2.909252586649425\n",
      "2.8941816060631362\n",
      "2.8784249258392\n",
      "2.866644007446122\n",
      "2.854349370451941\n",
      "2.8503177311780643\n",
      "2.8389473574502126\n",
      "2.826050909698432\n",
      "2.813475552579047\n",
      "2.7986462762305786\n",
      "2.789072677079174\n",
      "2.7748735218212524\n",
      "2.7623011269798017\n",
      "2.754136370558317\n",
      "2.7394299193008527\n",
      "2.7261916186985555\n",
      "2.7130688313643136\n",
      "2.7008443206351322\n",
      "2.687692019108095\n",
      "2.675856779603397\n",
      "2.664631305189876\n",
      "2.6592714421210752\n",
      "2.649526363381973\n",
      "2.6370355972818507\n",
      "2.6242707880991922\n",
      "2.612426265230719\n",
      "2.6008115828037264\n",
      "2.5893708460079217\n",
      "2.58138511504656\n",
      "2.569694694565849\n",
      "2.558829847995828\n",
      "2.5474215644778626\n",
      "2.5362978423216256\n",
      "2.526416586187785\n",
      "2.515574692615441\n",
      "2.50472485172678\n",
      "2.4954791549374074\n",
      "2.4847169061850387\n",
      "2.474154068287029\n",
      "2.4637290835380554\n",
      "2.4539568992866867\n",
      "2.444017810140337\n",
      "2.434676429764791\n",
      "2.4248850827163224\n",
      "2.4151552748144343\n",
      "2.405776987195681\n",
      "2.3966509342193603\n",
      "2.387086165544078\n",
      "2.3777942012299547\n",
      "2.368600600078458\n",
      "2.3593392106501954\n",
      "2.35022601469143\n",
      "2.3413474348283585\n",
      "2.332493860135104\n",
      "2.3237174102600586\n",
      "2.315366719134901\n",
      "2.3068508380337764\n",
      "2.298618910512375\n",
      "2.2904108871395388\n",
      "2.2819452186940246\n",
      "2.2732563491949103\n",
      "2.265087547363379\n",
      "2.257077861501246\n",
      "2.248589100874015\n",
      "2.240808821386761\n",
      "2.2338732678686553\n",
      "2.225355179011822\n",
      "2.2180646026312414\n",
      "2.2104938508260368\n",
      "2.2030341070273827\n",
      "2.1955119993756798\n",
      "2.1881417085484762\n",
      "2.180369518624926\n",
      "2.173241400776278\n",
      "2.1661142107958975\n",
      "2.159143859404696\n",
      "2.1520850587458837\n",
      "2.145399950126901\n",
      "2.1384659907165564\n",
      "2.1317351331733203\n",
      "2.125118706827966\n",
      "2.1184313516284146\n",
      "2.1125048961904316\n",
      "2.1059490376353813\n",
      "2.0999267650853604\n",
      "2.093434300052521\n",
      "2.0870405527678404\n",
      "2.0807243933505064\n",
      "2.074425924737174\n",
      "2.0682837289545035\n",
      "2.062035950699023\n",
      "2.0558090832498337\n",
      "2.0499640985400274\n",
      "2.0440574251607653\n",
      "2.0383363609251224\n",
      "2.0324653415700755\n",
      "2.026650959512462\n",
      "2.0218465756544304\n",
      "2.0168122572117837\n",
      "2.0116598951458418\n",
      "2.0063788557154503\n",
      "2.0006785826480136\n",
      "1.9952136636790583\n",
      "1.9898921595847054\n",
      "1.9845033969197954\n",
      "1.9789409512755263\n",
      "1.973461840301752\n",
      "1.9680061975950027\n",
      "1.9626382453875109\n",
      "1.9575116455309676\n",
      "1.9563202484220754\n",
      "1.9510836180375546\n",
      "1.94635827609194\n",
      "1.9412824427067992\n",
      "1.936383306980133\n",
      "1.9316828128803207\n",
      "1.9265366423130035\n",
      "1.923432933619298\n",
      "1.9197428207548837\n",
      "1.9147673319922134\n",
      "1.9103720779494038\n",
      "1.905631667959924\n",
      "1.9008621799293905\n",
      "1.8962159534836558\n",
      "1.8916852106881696\n",
      "1.9069227585461148\n",
      "1.9026206938120036\n",
      "1.8979926801275933\n",
      "1.8944563758737258\n",
      "1.8907194706423653\n",
      "1.9037628090291312\n",
      "1.8991872726746326\n",
      "1.8946748062183982\n",
      "1.890094776948293\n",
      "1.88784440751396\n",
      "1.8849827066673221\n",
      "1.8805276530760306\n",
      "1.8763081607343526\n",
      "1.8780237911816906\n",
      "1.8748818345558949\n",
      "1.8705396112734385\n",
      "1.8661502710255709\n",
      "1.8623360835987588\n",
      "1.8612451243486645\n",
      "1.8572711650844957\n",
      "1.8539419426285666\n",
      "1.8498950132301875\n",
      "1.8457466319362463\n",
      "1.8417006898010877\n",
      "1.8375006356003427\n",
      "1.8340137214727805\n",
      "1.8301127088697333\n",
      "1.8265014026131663\n",
      "1.8228855666798582\n",
      "1.8191899557908375\n",
      "1.8153032367617201\n",
      "1.8113939809388129\n",
      "1.8075126954370349\n",
      "1.803743881313768\n",
      "1.7999587598107374\n",
      "1.7960971289751482\n",
      "1.792602689791534\n",
      "1.788855428429874\n",
      "1.7854872095063077\n",
      "1.7818110423200082\n",
      "1.7782581759535747\n",
      "1.7746244166294733\n",
      "1.771050389818971\n",
      "1.767790075761593\n",
      "1.7644202414125498\n",
      "1.760788400510424\n",
      "1.7573472753899997\n",
      "1.7538176739917082\n",
      "1.7503747689607478\n",
      "1.746976277464396\n",
      "1.7435406733870893\n",
      "1.7400555925984535\n",
      "1.7365947434756535\n",
      "1.7333242845458863\n",
      "1.7299646502866532\n",
      "1.7267412664783988\n",
      "1.7234687778684827\n",
      "1.7202567214830011\n",
      "1.717307062743214\n",
      "1.7139034659232733\n",
      "1.71064238880869\n",
      "1.707477218657732\n",
      "1.704260753136929\n",
      "1.7011224218407033\n",
      "1.6980303721531256\n",
      "1.6953026282566566\n",
      "1.6922172856330873\n",
      "1.6888986295717625\n",
      "1.685928442244865\n",
      "1.6828129389664022\n",
      "1.679851352262642\n",
      "1.6770752643093918\n",
      "1.6742719606330387\n",
      "1.6712844307164112\n",
      "1.668624901377761\n",
      "1.6656898852593884\n",
      "1.6627044280963157\n",
      "1.6598054002083482\n",
      "1.6569547877821087\n",
      "1.654029893980929\n",
      "1.6511980567709885\n",
      "1.648386081702569\n",
      "1.6453838820331608\n",
      "1.6428647114519488\n",
      "1.6406102062314314\n",
      "1.6378526344548825\n",
      "1.6350982204727504\n",
      "1.6323874728872596\n",
      "1.629749109490804\n",
      "1.6270505967496456\n",
      "1.62562332248961\n",
      "1.6229606788499014\n",
      "1.6204575023420176\n",
      "1.6178083667023615\n",
      "1.615319345389142\n",
      "1.6127944305791693\n",
      "1.6101670057001247\n",
      "1.607616623800792\n",
      "1.6050466799936374\n",
      "1.6025110166832055\n",
      "1.599944614268279\n",
      "1.597460529042615\n",
      "1.5950449518549805\n",
      "1.5927345565010829\n",
      "1.5896514090132121\n",
      "1.5876234505366493\n",
      "1.585157616089468\n",
      "1.5826847685979364\n",
      "1.58029285641717\n",
      "1.577885140300445\n",
      "1.575911655739394\n",
      "1.5734670061517406\n",
      "1.5712790343800003\n",
      "1.5690056453308752\n",
      "1.5675385745377068\n",
      "1.5652021122648117\n",
      "1.5627980732123057\n",
      "1.5604745125041364\n",
      "1.5581610543500206\n",
      "1.5557813607353381\n",
      "1.5536183489186783\n",
      "1.551325954183152\n",
      "1.5490705700214766\n",
      "1.546791306262865\n",
      "1.5452328840217142\n",
      "1.543008485576138\n",
      "1.54182266391717\n",
      "1.539696245073037\n",
      "1.53747755928249\n",
      "1.5352931846816515\n",
      "1.5333043474004937\n",
      "1.5311501836929566\n",
      "1.52901489845932\n",
      "1.5270469308051529\n",
      "1.524902783656545\n",
      "1.5230008499120093\n",
      "1.5209019892577884\n",
      "1.5189256378346019\n",
      "1.5176921328459339\n",
      "1.5156263332420856\n",
      "1.5135697528981327\n",
      "1.5115574469417334\n",
      "1.5107268379662102\n",
      "1.5090841261753396\n",
      "1.507032110214825\n",
      "1.5051523347449776\n",
      "1.5031074854326836\n",
      "1.501134576542037\n",
      "1.4992523189698157\n",
      "1.4972824146788495\n",
      "1.4953387502790372\n",
      "1.4933048328975351\n",
      "1.4919301199651982\n",
      "1.4899815606694777\n",
      "1.4880815237111098\n",
      "1.4860039804724678\n",
      "1.4840239291449626\n",
      "1.4821275398851588\n",
      "1.4804109056933608\n",
      "1.478505969974413\n",
      "1.4766304516422435\n",
      "1.4747220958272615\n",
      "1.4728595302252192\n",
      "1.4710911869014045\n",
      "1.4693755260173311\n",
      "1.4675181670430697\n",
      "1.4653846481968376\n",
      "1.4635772133517153\n",
      "1.4617902235208686\n",
      "1.4600038066784913\n",
      "1.4583612532465609\n",
      "1.4566552259894305\n",
      "1.4549081010912521\n",
      "1.453123461800041\n",
      "1.451648478901689\n",
      "1.4499633348619883\n",
      "1.4482095235380632\n",
      "1.4465131526556583\n",
      "1.4460841108377793\n",
      "1.4445375898793407\n",
      "1.4428809797709514\n",
      "1.4411727798933334\n",
      "1.4394967236351264\n",
      "1.4382174758754704\n",
      "1.438926477666364\n",
      "1.437720328905024\n",
      "1.436133979344636\n",
      "1.4343724488009253\n",
      "1.4327548954977551\n",
      "1.4311233856715262\n",
      "1.4331738579113924\n",
      "1.4323317283391952\n",
      "1.4312847764819794\n",
      "1.4295978228998396\n",
      "1.4280764923058862\n",
      "1.4265750716830141\n",
      "1.4254077196776211\n",
      "1.4242137901223542\n",
      "1.423223214611183\n",
      "1.4216395593756672\n",
      "1.4200387722396643\n",
      "1.4185216283668642\n",
      "1.4172875187531468\n",
      "1.4159736177234938\n",
      "1.4143765797239143\n",
      "1.4129242503309045\n",
      "1.4113818316690383\n",
      "1.409857969235453\n",
      "1.40880112774367\n",
      "1.4072743606490967\n",
      "1.406609079794589\n",
      "1.4050916556982285\n",
      "1.4035830265524027\n",
      "1.4020749035654432\n",
      "1.4005480457183928\n",
      "1.4003076351518873\n",
      "1.3988884561312827\n",
      "1.3975756271671849\n",
      "1.3962904196740196\n",
      "1.3949322070162666\n",
      "1.3935434516511531\n",
      "1.3921196011826396\n",
      "1.3906381434437638\n",
      "1.3890854073881609\n",
      "1.3876594995489773\n",
      "1.3862003618158585\n",
      "1.3848515868801432\n",
      "1.3834368005824187\n",
      "1.3819643083416706\n",
      "1.3805445927943363\n",
      "1.3791252283840336\n",
      "1.3777670339053991\n",
      "1.3763550571535852\n",
      "1.374968875290417\n",
      "1.3737167663433973\n",
      "1.3726298409072977\n",
      "1.371269024803181\n",
      "1.3698902201628493\n",
      "1.3685968159070196\n",
      "1.3672344093940345\n",
      "1.3658572831349765\n",
      "1.3644924437403678\n",
      "1.363175240046012\n",
      "1.361867548638131\n",
      "1.3605137011758375\n",
      "1.359179993884431\n",
      "1.3578664026047924\n",
      "1.356585153183447\n",
      "1.355293064839271\n",
      "1.3539949875530295\n",
      "1.3526967310132587\n",
      "1.3513442207200854\n",
      "1.3500501649370398\n",
      "1.3486774297780357\n",
      "1.3473986029973504\n",
      "1.3461451972852885\n",
      "1.3448906691907678\n",
      "1.343601577677006\n",
      "1.3425631115584125\n",
      "1.341339547834341\n",
      "1.3400817512891656\n",
      "1.3388196053986365\n",
      "1.3376519422041477\n",
      "1.3362555845829718\n",
      "1.3349769475013302\n",
      "1.3337786180031208\n",
      "1.3325472934473128\n",
      "1.3313472549158358\n",
      "1.3303360210084825\n",
      "1.329136971102068\n",
      "1.327923892858123\n",
      "1.3267331424186815\n",
      "1.325486766472599\n",
      "1.324295853593744\n",
      "1.323094765252065\n",
      "1.3219404737935978\n",
      "1.3206869793272464\n",
      "1.319513312527048\n",
      "1.3183491092931403\n",
      "1.3172184633831996\n",
      "1.3160725836961742\n",
      "1.3149211050735579\n",
      "1.313776753581165\n",
      "1.3125546687313552\n",
      "1.3113750172693206\n",
      "1.3100242038969607\n",
      "1.3088729283678422\n",
      "1.3077081836514421\n",
      "1.3066053603532328\n",
      "1.3054448066510422\n",
      "1.304361364283414\n",
      "1.3032431037317622\n",
      "1.3021320246849648\n",
      "1.3010282828000144\n",
      "1.2999088526526583\n",
      "1.2988786664572864\n",
      "1.2978128293076077\n",
      "1.2967538877035216\n",
      "1.2956572840093925\n",
      "1.2945567709677535\n",
      "1.2934754612940582\n",
      "1.2926360478890793\n",
      "1.2915629091641057\n",
      "1.290504281790231\n",
      "1.2894309735319331\n",
      "1.2883724778996293\n",
      "1.2876216876295816\n",
      "1.2865541827447002\n",
      "1.2854875707226876\n",
      "1.2844396465995782\n",
      "1.2834274037127336\n",
      "1.2823484908593328\n",
      "1.2811294925087597\n",
      "1.2803796516744408\n",
      "1.2795932834373094\n",
      "1.2796438008754512\n",
      "1.2786023219253706\n",
      "1.2774827474624746\n",
      "1.2765792458991243\n",
      "1.2759879381495776\n",
      "1.277068592466427\n",
      "1.276151409385533\n",
      "1.2751467570072608\n",
      "1.2741328368789142\n",
      "1.2731830936251127\n",
      "1.2727412885080462\n",
      "1.2771182220715742\n",
      "1.276099103048393\n",
      "1.275137656135088\n",
      "1.276808492737968\n",
      "1.276750942391734\n",
      "1.278512478026293\n",
      "1.278174258882019\n",
      "1.277192143421318\n",
      "1.2762446928687683\n",
      "1.2785263096142296\n",
      "1.2777521480532255\n",
      "1.2781503323040553\n",
      "1.2772632002131623\n",
      "1.2769703826677041\n",
      "1.2770107482530437\n",
      "1.2765385963022708\n",
      "1.275576222309852\n",
      "1.2751359818286674\n",
      "1.2743000814669563\n",
      "1.2736034336567714\n",
      "1.272784314717143\n",
      "1.2717753909897096\n",
      "1.2708018434401203\n",
      "1.2698271290742253\n",
      "1.268870797797377\n",
      "1.2678958797552546\n",
      "1.2669462991326608\n",
      "1.2660859449723967\n",
      "1.2653087910878522\n",
      "1.2645027746692155\n",
      "1.263569856029216\n",
      "1.262607715298216\n",
      "1.261846984852848\n",
      "1.2614788886123491\n",
      "1.2605827681371198\n",
      "1.260062348506143\n",
      "1.2592031757516753\n",
      "1.258333241010019\n",
      "1.2573523400396251\n",
      "1.2566266083277953\n",
      "1.2557185309886931\n",
      "1.2553983553053853\n",
      "1.2545192984587838\n",
      "1.2537100531018464\n",
      "1.2530782884653118\n",
      "1.2521737509776676\n",
      "1.2513345615516185\n",
      "1.2506599029691159\n",
      "1.249785039803428\n",
      "1.248806863694161\n",
      "1.2480735186047442\n",
      "1.2473741497539874\n",
      "1.2464712796911153\n",
      "1.2455147885510167\n",
      "1.24457201417623\n",
      "1.243658438930288\n",
      "1.242757375033895\n",
      "1.2417738885615845\n",
      "1.2408795779915658\n",
      "1.240034977147668\n",
      "1.239280357055886\n",
      "1.2383156773567938\n",
      "1.2376107033198178\n",
      "1.2368059508318519\n",
      "1.2359843190260036\n",
      "1.2351950130554346\n",
      "1.234452898738571\n",
      "1.2336083917987126\n",
      "1.232775178154614\n",
      "1.2321232690814803\n",
      "1.2313031369948204\n",
      "1.2304464207462422\n",
      "1.2296283440593534\n",
      "1.2288177406081313\n",
      "1.2280015059505138\n",
      "1.2276165715221203\n",
      "1.2268363935774285\n",
      "1.2260510506856839\n",
      "1.225841794573164\n",
      "1.225222196878798\n",
      "1.2244135477937255\n",
      "1.2236106291458055\n",
      "1.2228380262494265\n",
      "1.222179315270421\n",
      "1.2218782950320763\n",
      "1.2211602174079241\n",
      "1.22087530838395\n",
      "1.2201471261768824\n",
      "1.2194764430331617\n",
      "1.2186694506456308\n",
      "1.2178913504547544\n",
      "1.2170841771029157\n",
      "1.2162693341426343\n",
      "1.2154775809134002\n",
      "1.2147065690322196\n",
      "1.2138958472977666\n",
      "1.21666164838445\n",
      "1.2159830937025484\n",
      "1.2152017000217215\n",
      "1.2144426181912422\n",
      "1.2183610538061518\n",
      "1.217558881935851\n",
      "1.2176381052407983\n",
      "1.2168329683414032\n",
      "1.2204080752174118\n",
      "1.2203352069077285\n",
      "1.2195615344471937\n",
      "1.219429634253069\n",
      "1.2239347681252644\n",
      "1.2233142204775933\n",
      "1.2230528992285832\n",
      "1.2224023201256649\n",
      "1.221667626025516\n",
      "1.2209880832358555\n",
      "1.2203121987279393\n",
      "1.2207697460055351\n",
      "1.2200956782073675\n",
      "1.2196850881532386\n",
      "1.2192532760574673\n",
      "1.218664589211006\n",
      "1.2180603442885352\n",
      "1.21762655903361\n",
      "1.2169653680149197\n",
      "1.2163222215354106\n",
      "1.2156751996959718\n",
      "1.214971609728437\n",
      "1.214607103674053\n",
      "1.213870825015762\n",
      "1.213355623614069\n",
      "1.215866174809739\n",
      "1.2152335954296005\n",
      "1.2148087189100973\n",
      "1.2142284428142271\n",
      "1.216570364025974\n",
      "1.2169498208086413\n",
      "1.2196174076447883\n",
      "1.2188614212110866\n",
      "1.2183082825671934\n",
      "1.2176866368551638\n",
      "1.2169313427904693\n",
      "1.2164080560207366\n",
      "1.2156857135949384\n",
      "1.214949085960034\n",
      "1.214216205485902\n",
      "1.2137457091867188\n",
      "1.2129617198281093\n",
      "1.2123107843702372\n",
      "1.212976797397345\n",
      "1.2128918320957018\n",
      "1.212442762479795\n",
      "1.2117659377808474\n",
      "1.2123792330612955\n",
      "1.2116680166695464\n",
      "1.2135246012026701\n",
      "1.2128210031857187\n",
      "1.2122914341253204\n",
      "1.2121245947524442\n",
      "1.2117403470521988\n",
      "1.2111860174750897\n",
      "1.2105290710685714\n",
      "1.2166986593864109\n",
      "1.2160442428758254\n",
      "1.2153343014528792\n",
      "1.2147425418311262\n",
      "1.214334828075325\n",
      "1.218026575922966\n",
      "1.2206791957192986\n",
      "1.2204221133935325\n",
      "1.2216717996207842\n",
      "1.2214026448344046\n",
      "1.2206847318355611\n",
      "1.2217332986848695\n",
      "1.2227561942731058\n",
      "1.2226326897701048\n",
      "1.221941366222379\n",
      "1.221454591382491\n",
      "1.2208087823785714\n",
      "1.2203307382156217\n",
      "1.2196703016133015\n",
      "1.2190829488541444\n",
      "1.2184007655171787\n",
      "1.2229567543255753\n",
      "1.2223988014918108\n",
      "1.2217233989310141\n",
      "1.2211112437772194\n",
      "1.220957776749289\n",
      "1.2204573183440048\n",
      "1.2198724519326278\n",
      "1.219224550398633\n",
      "1.2189392749730315\n",
      "1.2182123177666817\n",
      "1.2198913779071312\n",
      "1.2198783412019565\n",
      "1.2191632267058967\n",
      "1.2185019854939183\n",
      "1.2178042509234868\n",
      "1.2171944723620762\n",
      "1.219950829968428\n",
      "1.2199112430005543\n",
      "1.2193450297095947\n",
      "1.219094100461644\n",
      "1.218455616261516\n",
      "1.2193085392593883\n",
      "1.2189589165869703\n",
      "1.2192565355690712\n",
      "1.2196992865468883\n",
      "1.2218233668321905\n",
      "1.222854036009974\n",
      "1.222554445229142\n",
      "1.2232558336531185\n",
      "1.222795485063169\n",
      "1.2236761362034472\n",
      "1.2265228310896732\n",
      "1.2329729202023723\n",
      "1.2324152953260683\n",
      "1.2316903918609023\n",
      "1.231848821323016\n",
      "1.2311330796997446\n",
      "1.231056868473293\n",
      "1.2370866816954234\n",
      "1.2444142757365422\n",
      "1.247247382776613\n",
      "1.2465549877658948\n",
      "1.245955008128197\n",
      "1.2515187611612313\n",
      "1.2508948533991238\n",
      "1.254442004071211\n",
      "1.2576058840590159\n",
      "1.2586579732276124\n",
      "1.262993803078478\n",
      "1.2622345135621498\n",
      "1.26160914155052\n",
      "1.2631316320330015\n",
      "1.288163314639502\n",
      "1.2962747040467384\n",
      "1.2957081144539322\n",
      "1.2949747082013887\n",
      "1.3053058813305667\n",
      "1.3091144817785099\n",
      "1.322397600208382\n",
      "1.321736629334363\n",
      "1.322255014904186\n",
      "1.3218273618129028\n",
      "1.322406807275498\n",
      "1.3321167774525424\n",
      "1.3434153320200473\n",
      "1.3493727253734276\n",
      "1.3502057597685893\n",
      "1.3496398473916507\n",
      "1.3492657053742192\n",
      "1.3497303096477142\n",
      "1.3531465669068994\n",
      "1.3535769537287088\n",
      "1.3527704814097763\n",
      "1.3723562935609215\n",
      "1.3763382796375525\n",
      "1.3758428586153015\n",
      "1.3753732690264664\n",
      "1.37759145422621\n",
      "1.3770265161496769\n",
      "1.38155702165598\n",
      "1.3809732255431213\n",
      "1.3892494022423993\n",
      "1.3939369548955616\n",
      "1.3930948301184445\n",
      "1.4000255720405017\n",
      "1.399280190222692\n",
      "1.4243657517950858\n",
      "1.4237372926572842\n",
      "1.424388746990532\n",
      "1.4329596669701805\n",
      "1.4537970217240748\n",
      "1.4832050029357924\n",
      "1.4848887753306013\n",
      "1.494195958614627\n",
      "1.4932995131195979\n",
      "1.492572662756972\n",
      "1.4943962328464415\n",
      "1.4954887242126575\n",
      "1.5370329126232751\n",
      "1.5364809826274828\n",
      "1.5408172124596722\n",
      "1.5407543206091159\n",
      "1.545202872342503\n",
      "1.5470458645335001\n",
      "1.5565740836420279\n",
      "1.5556307383369223\n",
      "1.555903367185538\n",
      "1.5554807580418604\n",
      "1.55533458244746\n",
      "1.5546644858973366\n",
      "1.5536855422810876\n",
      "1.556051847634321\n",
      "1.554855115015849\n",
      "1.5553347442947556\n",
      "1.5545418368483133\n",
      "1.5538081232792402\n",
      "1.554677928149565\n",
      "1.554809237964291\n",
      "1.553834443751773\n",
      "1.5557075015905886\n",
      "1.5586136528721781\n",
      "1.5579120513081148\n",
      "1.5573721837822918\n",
      "1.5575386027986744\n",
      "1.5592778121152622\n",
      "1.5588119885937533\n",
      "1.5577361474456808\n",
      "1.556788723419388\n",
      "1.5583837855382254\n",
      "1.5576767378346215\n",
      "1.5569755191182983\n",
      "1.55672107715006\n",
      "1.5630878527007814\n",
      "1.5620664410848373\n",
      "1.5631507249342071\n",
      "1.563332664417506\n",
      "1.5632324161590334\n",
      "1.5653947900366016\n",
      "1.565323206538384\n",
      "1.5656657067122381\n",
      "1.5646561024665306\n",
      "1.5637505973515726\n",
      "1.5641509707081684\n",
      "1.564785404731207\n",
      "1.5653812055076872\n",
      "1.5644289229298267\n",
      "1.564223462323609\n",
      "1.56337037232071\n",
      "1.5638516190211151\n",
      "1.5629499450733102\n",
      "1.5677341769030522\n",
      "1.567574804683288\n",
      "1.5666234179324834\n",
      "1.5657456119963342\n",
      "1.565161661512178\n",
      "1.5643271589318004\n",
      "1.575493987075936\n",
      "1.5755174966894276\n",
      "1.5746556302750265\n",
      "1.5766876846068614\n",
      "1.5762473867791775\n",
      "1.5753480100515977\n",
      "1.5753660486882617\n",
      "1.5744794640846478\n",
      "1.5776895719510253\n",
      "1.5775157621992646\n",
      "1.576557645438311\n",
      "1.5762518184041925\n",
      "1.5753385657225227\n",
      "1.574546076747823\n",
      "1.574437092225521\n",
      "1.5736150097503478\n",
      "1.572596174726354\n",
      "1.5795273591979497\n",
      "1.5786024387529556\n",
      "1.5777294356118607\n",
      "1.5767985719973874\n",
      "1.575884702697412\n",
      "1.5749034684277692\n",
      "1.576987354812168\n",
      "1.5761778327866791\n",
      "1.5757728850010708\n",
      "1.5747522122937918\n",
      "1.5743602761415838\n",
      "1.5735115692176318\n",
      "1.5728293798009931\n",
      "1.5722139671507502\n",
      "1.5712901161512574\n",
      "1.5707042592439011\n",
      "1.569756952037362\n",
      "1.5688293802251876\n",
      "1.5678829178603342\n",
      "1.5669266537671795\n",
      "1.5659955297135462\n",
      "1.5651433709077538\n",
      "1.5649418552075662\n",
      "1.5642722108693727\n",
      "1.5633825491088698\n",
      "1.5625612265527\n",
      "1.5617547343123144\n",
      "1.5609418540329174\n",
      "1.5600539640347244\n",
      "1.5591438933283337\n",
      "1.5583999693024637\n",
      "1.5588282974417678\n",
      "1.5581866068464352\n",
      "1.5571902675454508\n",
      "1.5562114540070204\n",
      "1.5555974375724302\n",
      "1.5552960336819672\n",
      "1.5544325080013177\n",
      "1.5580073370233163\n",
      "1.5571036617022107\n",
      "1.556967807771967\n",
      "1.556096943391829\n",
      "1.555206284640883\n",
      "1.5543523956765468\n",
      "1.5534422756878552\n",
      "1.5552112741501836\n",
      "1.5542780438352963\n",
      "1.5543853810121273\n",
      "1.5534883879781856\n",
      "1.5526894296289455\n",
      "1.553171887466471\n",
      "1.5524672904098877\n",
      "1.5517920368434683\n",
      "1.5509674462880338\n",
      "1.5501320978245827\n",
      "1.5493510218273465\n",
      "1.5487265203765888\n",
      "1.5482841613122738\n",
      "1.547389938127074\n",
      "1.5472361255802947\n",
      "1.547760557991129\n",
      "1.5468936040699481\n",
      "1.5460550128579973\n",
      "1.5452370593350329\n",
      "1.5452062180070791\n",
      "1.5479380834803163\n",
      "1.5471291520405765\n",
      "1.5469731124151298\n",
      "1.5461269139710818\n",
      "1.5454690302943899\n",
      "1.5446510236078967\n",
      "1.5440419222163682\n",
      "1.543909407776258\n",
      "1.5430551071470906\n",
      "1.5422387176946262\n",
      "1.5422204021342407\n",
      "1.5414481971945082\n",
      "1.5406605626830434\n",
      "1.5399486939170954\n",
      "1.5391379529518792\n",
      "1.5389731371660111\n",
      "1.540121235245583\n",
      "1.5393730633464313\n",
      "1.5386866113852615\n",
      "1.5382501132502233\n",
      "1.5373089711356442\n",
      "1.5364309292014051\n",
      "1.5355747207040675\n",
      "1.5361981311524622\n",
      "1.5354364021800835\n",
      "1.5356558555822215\n",
      "1.5350080217261917\n",
      "1.534928767138841\n",
      "1.5346603177834388\n",
      "1.5338448900278632\n",
      "1.5338472393368383\n",
      "1.5330396808000002\n",
      "1.5323459229586667\n",
      "1.5317917320457006\n",
      "1.5316683863341694\n",
      "1.531076309569646\n",
      "1.5326361283946497\n",
      "1.5323347147844024\n",
      "1.5315555172495101\n",
      "1.5307858214568086\n",
      "1.5299995073698947\n",
      "1.5291945239288385\n",
      "1.5284125414849696\n",
      "1.5276466160301037\n",
      "1.5269919080759278\n",
      "1.5267428864003136\n",
      "1.526267554731596\n",
      "1.5255054873755498\n",
      "1.524767048600282\n",
      "1.528668960756273\n",
      "1.5280489388012117\n",
      "1.5276991072706703\n",
      "1.5269309233478976\n",
      "1.5269995629787445\n",
      "1.5299183381246033\n",
      "1.5291438444083991\n",
      "1.5283709009582143\n",
      "1.527692451974786\n",
      "1.5268961398558878\n",
      "1.5261457102374996\n",
      "1.5254044041205617\n",
      "1.5246294726508325\n",
      "1.5238564020305816\n",
      "1.5247530335282877\n",
      "1.5242356879825003\n",
      "1.5258626639842987\n",
      "1.5256293192923627\n",
      "1.5248394426233183\n",
      "1.5241817625434095\n",
      "1.5234659974917228\n",
      "1.5228735640006803\n",
      "1.522352088357127\n",
      "1.5268209948027887\n",
      "1.52626192970971\n",
      "1.5285813124204168\n",
      "1.5282396208753841\n",
      "1.5387018322116799\n",
      "1.5378091779988083\n",
      "1.5371751556389874\n",
      "1.5403903523805729\n",
      "1.566760140179048\n",
      "1.5685449339003057\n",
      "1.576462838119565\n",
      "1.5874563989939774\n",
      "1.5879487912718426\n",
      "1.5871618697958545\n",
      "1.5865817204254482\n",
      "1.589192939695145\n",
      "1.613436565594577\n",
      "1.616744163024349\n",
      "1.6237015805634545\n",
      "1.6285235434090166\n",
      "1.6291406829807444\n",
      "1.6298266086071975\n",
      "1.6291302382566022\n",
      "1.6309991849076217\n",
      "1.6302700537714092\n",
      "1.6358233884883728\n",
      "1.6453965658663405\n",
      "1.6458344159889307\n",
      "1.64525465718538\n",
      "1.6443916821102211\n",
      "1.6455843627291173\n",
      "1.6492830345570342\n",
      "1.6485250017434252\n",
      "1.647657829082367\n",
      "1.6499426218571964\n",
      "1.6494061270616378\n",
      "1.6485564481708215\n",
      "1.6478537178907111\n",
      "1.6476909826698594\n",
      "1.6502799602100133\n",
      "1.6502740664347526\n",
      "1.6498875450693078\n",
      "1.649003318655896\n",
      "1.6481319387594007\n",
      "1.6473373547994665\n",
      "1.6475874976480145\n",
      "1.6466561299541205\n",
      "1.6463933284804955\n",
      "1.645558154429193\n",
      "1.6451415571371715\n",
      "1.6442802114607387\n",
      "1.6443882799032206\n",
      "1.6435533267650622\n",
      "1.6427774722593036\n",
      "1.642310669121489\n",
      "1.64269523086746\n",
      "1.642299891966181\n",
      "1.6417597529904988\n",
      "1.6410211842158904\n",
      "1.6402448288407094\n",
      "1.6394065619826736\n",
      "1.6385625507240245\n",
      "1.6384117123269868\n",
      "1.6375726666808443\n",
      "1.6367371792071743\n",
      "1.6359263228335785\n",
      "1.6353875528335988\n",
      "1.63456719089055\n",
      "1.6337614411396997\n",
      "1.6328686359928164\n",
      "1.6326427676862862\n",
      "1.6318859786652646\n",
      "1.6310586902582271\n",
      "1.6305228607928473\n",
      "1.6301971030494442\n",
      "1.629386068296681\n",
      "1.6301402931567281\n",
      "1.6297650908710433\n",
      "1.628934158325402\n",
      "1.6283831390209529\n",
      "1.6366295851895967\n",
      "1.6357845461821907\n",
      "1.6351996308421017\n",
      "1.6344616098636384\n",
      "1.6383607560447577\n",
      "1.6411959215498093\n",
      "1.64045310848859\n",
      "1.6397328045433797\n",
      "1.6405472422332288\n",
      "1.6438864342388677\n",
      "1.643987365872128\n",
      "1.6437044663365927\n",
      "1.6429231372751194\n",
      "1.6421457242262067\n",
      "1.6413421422752559\n",
      "1.6404765452664698\n",
      "1.639763940901276\n",
      "1.6390730386245789\n",
      "1.6419508969712704\n",
      "1.6423800049183217\n",
      "1.6430914465616755\n",
      "1.6422887526281627\n",
      "1.6416417843265323\n",
      "1.6409410660149588\n",
      "1.6401433127663902\n",
      "1.6396407728019555\n",
      "1.6409781217423793\n",
      "1.640220338650306\n",
      "1.6414302280172706\n",
      "1.6407313917005113\n",
      "1.6400514688047552\n",
      "1.6397876895920207\n",
      "1.6391455696437898\n",
      "1.6394475258078467\n",
      "1.6395754156743778\n",
      "1.638988941407424\n",
      "1.6385149059929704\n",
      "1.6377438245105425\n",
      "1.6370398739424583\n",
      "1.6363079123167812\n",
      "1.6355813954735678\n",
      "1.6348589221287888\n",
      "1.634106348845517\n",
      "1.6335215862091628\n",
      "1.632885745242238\n",
      "1.6322259040150813\n",
      "1.6314595196141974\n",
      "1.6307429324708576\n",
      "1.6299709102689626\n",
      "1.629431990592806\n",
      "1.6286826735458169\n",
      "1.627802050572935\n",
      "1.6270315165798002\n",
      "1.626226948079637\n",
      "1.625521345301108\n",
      "1.6247477247683495\n",
      "1.624000184434672\n",
      "1.6232332553091073\n",
      "1.6225271697050265\n",
      "1.6217674533280817\n",
      "1.6210506122412258\n",
      "1.6202800789864806\n",
      "1.6195007465498397\n",
      "1.6186939691842042\n",
      "1.6179547947205482\n",
      "1.6171392782070322\n",
      "1.6163988900409003\n",
      "1.615674503714163\n",
      "1.6149273335787595\n",
      "1.6142158612183162\n",
      "1.61346708286742\n",
      "1.612709591105981\n",
      "1.6119576171040535\n",
      "1.6111997133886262\n",
      "1.6113639612265718\n",
      "1.610698123296812\n",
      "1.6099766811912324\n",
      "1.6092596875151675\n",
      "1.6085206493131539\n",
      "1.6078100606738797\n",
      "1.607436931900029\n",
      "1.6120303757217689\n",
      "1.6112528102222667\n",
      "1.6104799366892106\n",
      "1.6097232432855715\n",
      "1.6106368099413602\n",
      "1.60988630944692\n",
      "1.6092028092542006\n",
      "1.608580841296547\n",
      "1.6078734019434595\n",
      "1.6074443847706383\n",
      "1.6067289702636103\n",
      "1.6069249878279293\n",
      "1.606223516434646\n",
      "1.605750682759285\n",
      "1.6049769300398686\n",
      "1.6042771811921375\n",
      "1.6035716524860522\n",
      "1.6028552965826965\n",
      "1.6021171629428863\n",
      "1.6016238103769007\n",
      "1.6021274391535074\n",
      "1.601608018877965\n",
      "1.6010200713389067\n",
      "1.6003879836390889\n",
      "1.6008239419898942\n",
      "1.6000863248733257\n",
      "1.5994065803426274\n",
      "1.598967601108966\n",
      "1.598266383663939\n",
      "1.5977887035028253\n",
      "1.5970933905669622\n",
      "1.596361160254817\n",
      "1.5956584889885006\n",
      "1.594981465494539\n",
      "1.5943234715050922\n",
      "1.5937336379392717\n",
      "1.5934914326864438\n",
      "1.5928674753129577\n",
      "1.5921484768390655\n",
      "1.5914456961343655\n",
      "1.5907520981579901\n",
      "1.589960416601111\n",
      "1.5893189769829503\n",
      "1.5886337421135976\n",
      "1.5880184251056064\n",
      "1.587319451519368\n",
      "1.5866420083880333\n",
      "1.5859145747027665\n",
      "1.585208692230603\n",
      "1.584504259573542\n",
      "1.5839239767612925\n",
      "1.5832588022678906\n",
      "1.5825679241408666\n",
      "1.5825102174235868\n",
      "1.5818211755071103\n",
      "1.5811206337883377\n",
      "1.5805353100735\n",
      "1.5798688102312028\n",
      "1.5799422822633766\n",
      "1.5792701688859565\n",
      "1.5785515643627532\n",
      "1.5778673449402048\n",
      "1.5773740573467154\n",
      "1.576924166931556\n",
      "1.5762397858795985\n",
      "1.5755630469953958\n",
      "1.5748827381263946\n",
      "1.5742330283063686\n",
      "1.5744330313699\n",
      "1.5738261259213706\n",
      "1.5731968873087103\n",
      "1.5726116047858827\n",
      "1.571907989413252\n",
      "1.5712737060002697\n",
      "1.5705545281609172\n",
      "1.5701350446381583\n",
      "1.56944070814315\n",
      "1.568764404278912\n",
      "1.5680925301045971\n",
      "1.5674352488936263\n",
      "1.5667852978635757\n",
      "1.5661546441193597\n",
      "1.5654574093310738\n",
      "1.5648643810866456\n",
      "1.5642175525314062\n",
      "1.5635588377623983\n",
      "1.5629350772114838\n",
      "1.5622782054219362\n",
      "1.5616062077711212\n",
      "1.5609595268784964\n",
      "1.560324466556378\n",
      "1.5596661582723799\n",
      "1.5590027106310569\n",
      "1.5583452821450126\n",
      "1.5577003880831464\n",
      "1.5570691406234607\n",
      "1.5564102199814505\n",
      "1.5557650476694107\n",
      "1.5551042593597026\n",
      "1.5544617170479127\n",
      "1.553767731520518\n",
      "1.5531225344124395\n",
      "1.5524952412764468\n",
      "1.5518677123669369\n",
      "1.5512590495665335\n",
      "1.5505991864355417\n",
      "1.5499669032272614\n",
      "1.5493266409307362\n",
      "1.5486882365547592\n",
      "1.5480541756257613\n",
      "1.5474083949082325\n",
      "1.546773051627489\n",
      "1.546142036535724\n",
      "1.5455077564054065\n",
      "1.5448797113316577\n",
      "1.5442868108976875\n",
      "1.5436550833653804\n",
      "1.543049171522942\n",
      "1.542414570756504\n",
      "1.5418392404210604\n",
      "1.5411932810800604\n",
      "1.5405486760021836\n"
     ]
    }
   ],
   "source": [
    "from numpy import dtype\n",
    "import torch\n",
    "from fs_mol.custom.utils import convert_to_pyg_graph\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "\n",
    "optm = Adam(model.parameters())\n",
    "\n",
    "losses = []\n",
    "\n",
    "def Average(lst):\n",
    "    return sum(lst) / len(lst)\n",
    "\n",
    "\n",
    "for task in train_iterable:\n",
    "    support_samples = task.train_samples\n",
    "    query_samples = task.test_samples\n",
    "\n",
    "    support_sample_graphs = list(map(lambda x: convert_to_pyg_graph(x, device=device), support_samples))\n",
    "\n",
    "    query_sample_graphs = list(map(lambda x: convert_to_pyg_graph(x, device=device), query_samples))\n",
    "\n",
    "    for batch in DataLoader(support_sample_graphs, batch_size=len(support_sample_graphs), shuffle=True):\n",
    "        optm.zero_grad()\n",
    "        encoded_support_examples = model(batch, batch.batch)\n",
    "\n",
    "        class_one_indices = (batch.y == 0).nonzero().squeeze(-1)\n",
    "        class_two_indices = (batch.y == 1).nonzero().squeeze(-1)\n",
    "\n",
    "        class_one_vectors = encoded_support_examples[class_one_indices]\n",
    "        class_two_vectors = encoded_support_examples[class_two_indices]\n",
    "\n",
    "        class_one_mean = class_one_vectors.mean(dim=0)\n",
    "        class_two_mean = class_two_vectors.mean(dim=0)\n",
    "\n",
    "        means = torch.vstack([class_one_mean, class_two_mean])\n",
    "\n",
    "        for query_batch in DataLoader(query_sample_graphs, batch_size=len(query_sample_graphs), shuffle=True):\n",
    "            encoded_query_examples = model(query_batch, query_batch.batch)\n",
    "\n",
    "            distances = (means @ encoded_query_examples.t()).t()\n",
    "\n",
    "            loss = F.cross_entropy(distances, query_batch.y)\n",
    "            loss.backward()\n",
    "\n",
    "            optm.step()\n",
    "            losses.append(loss.item())\n",
    "    \n",
    "    print(Average(losses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.metrics import PrecisionRecallDisplay, precision_recall_curve, auc\n",
    "\n",
    "\n",
    "def delta_calc_auc_pr(l, t):\n",
    "    z = Counter(l)\n",
    "    pos_percentage = z[1] / len(l)\n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(l, t)\n",
    "\n",
    "    return auc(recall, precision) - pos_percentage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 166, 1: 90})\n",
      "Counter({0: 119, 1: 119})\n",
      "Counter({0: 47, 1: 46})\n",
      "Counter({0: 47, 1: 46})\n",
      "Counter({0: 48, 1: 45})\n",
      "Counter({1: 128, 0: 128})\n",
      "Counter({0: 48, 1: 22})\n",
      "Counter({0: 81, 1: 81})\n",
      "Counter({0: 47, 1: 46})\n",
      "Counter({0: 47, 1: 46})\n",
      "Counter({0: 154, 1: 102})\n",
      "Counter({0: 141, 1: 115})\n",
      "Counter({0: 49, 1: 44})\n",
      "Counter({0: 47, 1: 46})\n",
      "Counter({1: 46, 0: 46})\n",
      "Counter({0: 49, 1: 44})\n",
      "Counter({0: 67, 1: 67})\n",
      "Counter({0: 157, 1: 99})\n",
      "Counter({0: 47, 1: 46})\n",
      "Counter({1: 88, 0: 88})\n",
      "Counter({0: 40, 1: 38})\n",
      "Counter({0: 36, 1: 35})\n",
      "Counter({0: 47, 1: 46})\n",
      "Counter({1: 128, 0: 128})\n",
      "Counter({0: 47, 1: 46})\n",
      "Counter({0: 38, 1: 38})\n",
      "Counter({0: 49, 1: 44})\n",
      "Counter({0: 128, 1: 128})\n",
      "Counter({0: 47, 1: 46})\n",
      "Counter({0: 49, 1: 44})\n",
      "Counter({0: 47, 1: 46})\n",
      "Counter({0: 166, 1: 90})\n",
      "Counter({0: 49, 1: 44})\n",
      "Counter({0: 47, 1: 46})\n",
      "Counter({0: 47, 1: 46})\n",
      "Counter({1: 128, 0: 128})\n",
      "Counter({0: 173, 1: 83})\n",
      "Counter({0: 49, 1: 44})\n",
      "Counter({1: 128, 0: 128})\n",
      "Counter({0: 49, 1: 44})\n",
      "Counter({0: 43, 1: 42})\n",
      "Counter({0: 48, 1: 45})\n",
      "Counter({0: 49, 1: 44})\n",
      "Counter({0: 47, 1: 46})\n",
      "Counter({1: 128, 0: 128})\n",
      "Counter({0: 129, 1: 127})\n",
      "Counter({0: 47, 1: 46})\n",
      "Counter({0: 93, 1: 91})\n",
      "Counter({0: 92, 1: 61})\n",
      "Counter({0: 128, 1: 128})\n",
      "Counter({0: 47, 1: 46})\n",
      "Counter({0: 50, 1: 43})\n",
      "Counter({0: 47, 1: 46})\n",
      "Counter({1: 47, 0: 47})\n",
      "Counter({0: 147, 1: 109})\n",
      "Counter({0: 46, 1: 45})\n",
      "Counter({0: 49, 1: 44})\n",
      "Counter({0: 139, 1: 117})\n",
      "Counter({0: 47, 1: 46})\n",
      "Counter({0: 132, 1: 124})\n",
      "Counter({0: 49, 1: 44})\n",
      "Counter({0: 48, 1: 45})\n",
      "Counter({0: 88, 1: 87})\n",
      "Counter({0: 48, 1: 45})\n",
      "Counter({0: 167, 1: 85})\n",
      "Counter({0: 133, 1: 123})\n",
      "Counter({0: 39, 1: 38})\n",
      "Counter({0: 47, 1: 46})\n",
      "Counter({0: 47, 1: 46})\n",
      "Counter({0: 52, 1: 23})\n",
      "Counter({0: 62, 1: 61})\n",
      "Counter({0: 47, 1: 46})\n",
      "Counter({0: 68, 1: 68})\n",
      "Counter({0: 47, 1: 46})\n",
      "Counter({0: 79, 1: 71})\n",
      "Counter({0: 47, 1: 46})\n",
      "Counter({0: 42, 1: 41})\n",
      "Counter({0: 140, 1: 116})\n",
      "Counter({0: 48, 1: 45})\n",
      "Counter({0: 47, 1: 46})\n",
      "Counter({0: 149, 1: 107})\n",
      "Counter({0: 47, 1: 46})\n",
      "Counter({0: 52, 1: 41})\n",
      "Counter({0: 78, 1: 77})\n",
      "Counter({0: 170, 1: 86})\n",
      "Counter({0: 42, 1: 40})\n",
      "Counter({0: 128, 1: 128})\n",
      "Counter({0: 149, 1: 107})\n",
      "Counter({1: 72, 0: 72})\n",
      "Counter({1: 72, 0: 72})\n",
      "Counter({0: 159, 1: 97})\n",
      "Counter({0: 128, 1: 128})\n",
      "Counter({0: 49, 1: 44})\n",
      "Counter({0: 49, 1: 44})\n",
      "Counter({0: 48, 1: 45})\n",
      "Counter({0: 40, 1: 39})\n",
      "Counter({0: 47, 1: 46})\n",
      "Counter({0: 46, 1: 45})\n",
      "Counter({0: 147, 1: 109})\n",
      "Counter({0: 48, 1: 45})\n",
      "Counter({0: 159, 1: 97})\n",
      "Counter({0: 130, 1: 126})\n",
      "Counter({0: 47, 1: 46})\n",
      "Counter({0: 157, 1: 99})\n",
      "Counter({0: 47, 1: 46})\n",
      "Counter({0: 47, 1: 46})\n",
      "Counter({0: 47, 1: 46})\n",
      "Counter({0: 47, 1: 46})\n",
      "Counter({0: 47, 1: 46})\n",
      "Counter({0: 47, 1: 46})\n",
      "Counter({0: 141, 1: 115})\n",
      "Counter({0: 62, 1: 32})\n",
      "Counter({0: 47, 1: 46})\n",
      "Counter({0: 166, 1: 90})\n",
      "Counter({1: 128, 0: 128})\n",
      "Counter({0: 47, 1: 46})\n",
      "Counter({0: 55, 1: 55})\n",
      "Counter({0: 149, 1: 107})\n",
      "Counter({0: 50, 1: 43})\n",
      "Counter({0: 88, 1: 88})\n",
      "Counter({0: 47, 1: 46})\n",
      "Counter({0: 47, 1: 46})\n",
      "Counter({0: 128, 1: 128})\n",
      "Counter({0: 56, 1: 49})\n",
      "Counter({0: 47, 1: 46})\n",
      "Counter({0: 68, 1: 32})\n",
      "Counter({0: 55, 1: 54})\n",
      "Counter({1: 49, 0: 49})\n",
      "Counter({0: 49, 1: 44})\n",
      "Counter({0: 128, 1: 128})\n",
      "Counter({0: 52, 1: 51})\n",
      "Counter({0: 86, 1: 82})\n",
      "Counter({1: 128, 0: 128})\n",
      "Counter({0: 147, 1: 109})\n",
      "Counter({0: 115, 1: 114})\n",
      "Counter({0: 81, 1: 48})\n",
      "Counter({0: 47, 1: 46})\n",
      "Counter({0: 157, 1: 99})\n",
      "Counter({0: 48, 1: 45})\n",
      "Counter({0: 48, 1: 45})\n",
      "Counter({0: 49, 1: 44})\n",
      "Counter({0: 49, 1: 44})\n",
      "Counter({0: 56, 1: 55})\n",
      "Counter({0: 128, 1: 128})\n",
      "Counter({0: 51, 1: 50})\n",
      "Counter({0: 49, 1: 42})\n",
      "Counter({0: 38, 1: 38})\n",
      "Counter({0: 49, 1: 44})\n",
      "Counter({0: 47, 1: 46})\n",
      "Counter({0: 48, 1: 45})\n",
      "Counter({1: 34, 0: 32})\n",
      "Counter({0: 47, 1: 46})\n",
      "Counter({0: 52, 1: 41})\n",
      "Counter({0: 47, 1: 46})\n",
      "Counter({0: 47, 1: 46})\n",
      "Counter({0: 140, 1: 116})\n",
      "Counter({0: 47, 1: 46})\n"
     ]
    }
   ],
   "source": [
    "test_task_iterable = generate_episodic_iterable(fsmol_dataset, DataFold.TEST)\n",
    "\n",
    "eval_metric = []\n",
    "\n",
    "\n",
    "for task in test_task_iterable:\n",
    "    support_samples = task.train_samples\n",
    "    query_samples = task.test_samples\n",
    "\n",
    "    support_sample_graphs = list(map(convert_to_pyg_graph, support_samples))\n",
    "\n",
    "    query_sample_graphs = list(map(convert_to_pyg_graph, query_samples))\n",
    "\n",
    "    for batch in DataLoader(support_sample_graphs, batch_size=len(support_sample_graphs), shuffle=True):\n",
    "        encoded_support_examples = model(batch, batch.batch)\n",
    "\n",
    "        class_one_indices = (batch.y == 0).nonzero().squeeze(-1)\n",
    "        class_two_indices = (batch.y == 1).nonzero().squeeze(-1)\n",
    "\n",
    "        class_one_vectors = encoded_support_examples[class_one_indices]\n",
    "        class_two_vectors = encoded_support_examples[class_two_indices]\n",
    "\n",
    "        class_one_mean = class_one_vectors.mean(dim=0)\n",
    "        class_two_mean = class_two_vectors.mean(dim=0)\n",
    "\n",
    "        means = torch.vstack([class_one_mean, class_two_mean])\n",
    "\n",
    "        for query_batch in DataLoader(query_sample_graphs, batch_size=len(query_sample_graphs), shuffle=True):\n",
    "            encoded_query_examples = model(query_batch, query_batch.batch)\n",
    "\n",
    "            distances = (means @ encoded_query_examples.t()).t()\n",
    "\n",
    "            softmax_result = F.softmax(distances, dim=1)\n",
    "\n",
    "            probabilities = softmax_result[range(softmax_result.shape[0]), query_batch.y].tolist()\n",
    "            \n",
    "            delta_auprc = delta_calc_auc_pr(query_batch.y.tolist(), probabilities)\n",
    "\n",
    "            eval_metric.append(delta_auprc)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12087584269324367"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Average(eval_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.19262290399512694\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2967fc0a0>]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from sklearn.metrics import PrecisionRecallDisplay, precision_recall_curve, auc\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "a = Counter(true_labels)\n",
    "\n",
    "pos_percentage = a[1] / len(true_labels)\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(true_labels, probs)\n",
    "\n",
    "print(auc(recall, precision) - pos_percentage)\n",
    "\n",
    "plt.plot(recall, precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1358"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4938"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(task_iterable._data_paths) // task_iterable._reader_chunk_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(task_iterable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'generator' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/athecoder/FS-Mol/train_simple.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/athecoder/FS-Mol/train_simple.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mlen\u001b[39;49m(iterator)\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'generator' has no len()"
     ]
    }
   ],
   "source": [
    "len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1713it [02:25, 11.79it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/athecoder/FS-Mol/train_simple.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/athecoder/FS-Mol/train_simple.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtqdm\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/athecoder/FS-Mol/train_simple.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m number_of_tasks \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/athecoder/FS-Mol/train_simple.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m datapoint \u001b[39min\u001b[39;00m tqdm(iterator):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/athecoder/FS-Mol/train_simple.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     number_of_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/FS-Mol/env/lib/python3.8/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/FS-Mol/fs_mol/data/file_reader_iterable.py:303\u001b[0m, in \u001b[0;36mSequentialFileReaderIterable.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reader_chunk_size):\n\u001b[1;32m    302\u001b[0m     chunk_paths\u001b[39m.\u001b[39mappend(input_paths\u001b[39m.\u001b[39mpop())\n\u001b[0;32m--> 303\u001b[0m \u001b[39mfor\u001b[39;00m reader_output \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reader_fn(chunk_paths, chunk_idx):\n\u001b[1;32m    304\u001b[0m     \u001b[39myield\u001b[39;00m reader_output\n\u001b[1;32m    305\u001b[0m chunk_idx \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[1;32m/Users/athecoder/FS-Mol/train_simple.ipynb Cell 5\u001b[0m in \u001b[0;36mgenerate_task_sample_iterable.<locals>.simple_task_reader\u001b[0;34m(paths, idx)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/athecoder/FS-Mol/train_simple.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msimple_task_reader\u001b[39m(paths: List[RichPath], idx: \u001b[39mint\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/athecoder/FS-Mol/train_simple.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     task \u001b[39m=\u001b[39m FSMolTask\u001b[39m.\u001b[39;49mload_from_file(paths[\u001b[39m0\u001b[39;49m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/athecoder/FS-Mol/train_simple.ipynb#W3sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/athecoder/FS-Mol/train_simple.ipynb#W3sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         sampled_task \u001b[39m=\u001b[39m task_sampler\u001b[39m.\u001b[39msample(task)\n",
      "File \u001b[0;32m~/FS-Mol/fs_mol/data/fsmol_task.py:108\u001b[0m, in \u001b[0;36mFSMolTask.load_from_file\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    106\u001b[0m fingerprint_raw \u001b[39m=\u001b[39m raw_sample\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mfingerprints\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    107\u001b[0m \u001b[39mif\u001b[39;00m fingerprint_raw \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 108\u001b[0m     fingerprint: Optional[np\u001b[39m.\u001b[39mndarray] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49marray(fingerprint_raw, dtype\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49mint32)\n\u001b[1;32m    109\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    110\u001b[0m     fingerprint \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "number_of_tasks = 0\n",
    "\n",
    "for datapoint in tqdm(iterator):\n",
    "    number_of_tasks += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'FSMolTaskSample' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/athecoder/FS-Mol/train_simple.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/athecoder/FS-Mol/train_simple.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m a \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(task_iter)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/athecoder/FS-Mol/train_simple.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mlen\u001b[39;49m(a)\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'FSMolTaskSample' has no len()"
     ]
    }
   ],
   "source": [
    "a = next(task_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphData(node_features=array([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.]],\n",
       "      dtype=float32), adjacency_lists=[array([[ 0,  1],\n",
       "       [ 1,  2],\n",
       "       [ 3,  4],\n",
       "       [ 5,  6],\n",
       "       [ 6,  7],\n",
       "       [ 8,  9],\n",
       "       [10, 11],\n",
       "       [10, 12],\n",
       "       [13, 14],\n",
       "       [ 5, 15],\n",
       "       [16, 17],\n",
       "       [18, 19],\n",
       "       [20, 21],\n",
       "       [21, 22],\n",
       "       [22, 23],\n",
       "       [23, 24],\n",
       "       [24, 25],\n",
       "       [25, 26],\n",
       "       [26, 27],\n",
       "       [26, 28],\n",
       "       [28, 29],\n",
       "       [20,  2],\n",
       "       [29, 23],\n",
       "       [18,  4],\n",
       "       [13,  7]]), array([[ 2,  3],\n",
       "       [ 4,  5],\n",
       "       [ 7,  8],\n",
       "       [ 9, 10],\n",
       "       [12, 13],\n",
       "       [15, 16],\n",
       "       [17, 18],\n",
       "       [19, 20]]), array([], shape=(0, 2), dtype=int64)], edge_features=[])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.train_samples[0].graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0,  1],\n",
       "        [ 1,  2],\n",
       "        [ 3,  4],\n",
       "        [ 5,  6],\n",
       "        [ 6,  7],\n",
       "        [ 8,  9],\n",
       "        [10, 11],\n",
       "        [10, 12],\n",
       "        [13, 14],\n",
       "        [ 5, 15],\n",
       "        [16, 17],\n",
       "        [18, 19],\n",
       "        [20, 21],\n",
       "        [21, 22],\n",
       "        [22, 23],\n",
       "        [23, 24],\n",
       "        [24, 25],\n",
       "        [25, 26],\n",
       "        [26, 27],\n",
       "        [26, 28],\n",
       "        [28, 29],\n",
       "        [20,  2],\n",
       "        [29, 23],\n",
       "        [18,  4],\n",
       "        [13,  7]]),\n",
       " array([[ 2,  3],\n",
       "        [ 4,  5],\n",
       "        [ 7,  8],\n",
       "        [ 9, 10],\n",
       "        [12, 13],\n",
       "        [15, 16],\n",
       "        [17, 18],\n",
       "        [19, 20]]),\n",
       " array([], shape=(0, 2), dtype=int64)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.train_samples[0].graph.adjacency_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[30, 32], edge_index=[2, 33], edge_attr=[33])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "def convert_to_pyg_graph(graph):\n",
    "    x = torch.tensor(graph.node_features)\n",
    "    adjacency_lists = graph.adjacency_lists\n",
    "\n",
    "    single_bonds = adjacency_lists[0]\n",
    "    double_bonds = adjacency_lists[1]\n",
    "    triple_bonds =  adjacency_lists[2]\n",
    "\n",
    "    edge_index = torch.cat(list(map(torch.tensor, adjacency_lists)), dim=0).t().contiguous()\n",
    "\n",
    "    edge_feats = [0 for bond in single_bonds] + [1 for bond in double_bonds] + [2 for bond in triple_bonds]\n",
    "\n",
    "    return Data(x=x, edge_index=edge_index, edge_attr=edge_feats)\n",
    "\n",
    "\n",
    "convert_to_pyg_graph(a.train_samples[0].graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 2)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.train_samples[0].graph.adjacency_lists[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = next(task_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "DatasetTooSmallException",
     "evalue": "Cannot satisfy request to split dataset as specified because the dataset is too small.\n  Task name: CHEMBL1261338\n  Number of samples: 51\n  Requested sample: 64 train, 0 valid, -13 test samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDatasetTooSmallException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/athecoder/FS-Mol/train_simple.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/athecoder/FS-Mol/train_simple.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m task_sampler \u001b[39m=\u001b[39m StratifiedTaskSampler(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/athecoder/FS-Mol/train_simple.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     train_size_or_ratio\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m, test_size_or_ratio\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/athecoder/FS-Mol/train_simple.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m )\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/athecoder/FS-Mol/train_simple.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m task_sampler\u001b[39m.\u001b[39;49msample(q)\n",
      "File \u001b[0;32m/notebooks/fs_mol/data/fsmol_task_sampler.py:364\u001b[0m, in \u001b[0;36mStratifiedTaskSampler.sample\u001b[0;34m(self, task, seed)\u001b[0m\n\u001b[1;32m    361\u001b[0m         num_test \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(num_test, possible_test_size)\n\u001b[1;32m    363\u001b[0m \u001b[39mif\u001b[39;00m num_test \u001b[39m<\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m--> 364\u001b[0m     \u001b[39mraise\u001b[39;00m DatasetTooSmallException(\n\u001b[1;32m    365\u001b[0m         task\u001b[39m.\u001b[39mname,\n\u001b[1;32m    366\u001b[0m         num_samples\u001b[39m=\u001b[39mnum_samples,\n\u001b[1;32m    367\u001b[0m         num_train\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_size_or_ratio,\n\u001b[1;32m    368\u001b[0m         num_valid\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,\n\u001b[1;32m    369\u001b[0m         num_test\u001b[39m=\u001b[39mnum_test,\n\u001b[1;32m    370\u001b[0m     )\n\u001b[1;32m    371\u001b[0m train_test_splitter_obj \u001b[39m=\u001b[39m StratifiedShuffleSplit(\n\u001b[1;32m    372\u001b[0m     n_splits\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, train_size\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_size_or_ratio, test_size\u001b[39m=\u001b[39mnum_test, random_state\u001b[39m=\u001b[39mseed\n\u001b[1;32m    373\u001b[0m )\n\u001b[1;32m    374\u001b[0m train_valid_idxs, test_idxs \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(train_test_splitter_obj\u001b[39m.\u001b[39msplit(X\u001b[39m=\u001b[39mindices, y\u001b[39m=\u001b[39mlabels)))\n",
      "\u001b[0;31mDatasetTooSmallException\u001b[0m: Cannot satisfy request to split dataset as specified because the dataset is too small.\n  Task name: CHEMBL1261338\n  Number of samples: 51\n  Requested sample: 64 train, 0 valid, -13 test samples."
     ]
    }
   ],
   "source": [
    "task_sampler = StratifiedTaskSampler(\n",
    "    train_size_or_ratio=64, test_size_or_ratio=256\n",
    ")\n",
    "\n",
    "task_sampler.sample(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "proto_net_task_sample_interable = get_protonet_task_sample_iterable(\n",
    "        dataset=fsmol_dataset,\n",
    "        data_fold=DataFold.TRAIN,\n",
    "        num_samples=1,\n",
    "        max_num_graphs=256,\n",
    "        support_size=64,\n",
    "        query_size=256,\n",
    "        repeat=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_task_sample_iterator = iter(proto_net_task_sample_interable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Positive Samples: 30\n",
      "Number of Negative Samples: 34\n"
     ]
    }
   ],
   "source": [
    "from fs_mol.utils.torch_utils import torchify\n",
    "\n",
    "\n",
    "task_sample = next(train_task_sample_iterator)\n",
    "\n",
    "example = torchify(task_sample, device='cuda')\n",
    "\n",
    "print(f'Number of Positive Samples: {example.num_positive_support_samples}')\n",
    "print(f'Number of Negative Samples: {example.num_support_samples - example.num_positive_support_samples}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProtoNetBatch(support_features=MoleculeProtoNetFeatures(num_graphs=64, num_nodes=1578, num_edges=1752, node_features=tensor([[0., 1., 0.,  ..., 1., 0., 0.],\n",
       "        [0., 0., 1.,  ..., 1., 0., 1.],\n",
       "        [0., 0., 1.,  ..., 1., 0., 1.],\n",
       "        ...,\n",
       "        [0., 1., 0.,  ..., 1., 0., 1.],\n",
       "        [0., 1., 0.,  ..., 1., 0., 1.],\n",
       "        [0., 1., 0.,  ..., 1., 0., 1.]], device='cuda:0'), adjacency_lists=[tensor([[   0,    1],\n",
       "        [   1,    2],\n",
       "        [   3,    4],\n",
       "        ...,\n",
       "        [1576, 1577],\n",
       "        [1577, 1563],\n",
       "        [1575, 1570]], device='cuda:0'), tensor([[   2,    3],\n",
       "        [   4,    5],\n",
       "        [   6,    7],\n",
       "        ...,\n",
       "        [1570, 1571],\n",
       "        [1572, 1573],\n",
       "        [1574, 1575]], device='cuda:0'), tensor([[ 320,  321],\n",
       "        [ 442,  443],\n",
       "        [ 531,  532],\n",
       "        [1043, 1044],\n",
       "        [1057, 1058],\n",
       "        [1059, 1060]], device='cuda:0')], edge_features=[tensor([], device='cuda:0', size=(1175, 0)), tensor([], device='cuda:0', size=(571, 0)), tensor([], device='cuda:0', size=(6, 0))], node_to_graph=tensor([ 0,  0,  0,  ..., 63, 63, 63], device='cuda:0'), fingerprints=tensor([[0, 1, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 1, 0,  ..., 0, 0, 0]], device='cuda:0', dtype=torch.int32), descriptors=tensor([[12.4667,  0.0942, 12.4667,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 5.8867,  0.7031,  5.8867,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [10.3035,  0.3134, 10.3035,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        ...,\n",
       "        [13.4530, -3.8816, 13.4530,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [11.2906, -0.4721, 11.2906,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [12.0813,  0.1290, 12.0813,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "       device='cuda:0')), support_labels=tensor([False, False, False,  True, False,  True, False, False,  True, False,\n",
       "        False,  True,  True,  True, False, False, False,  True,  True,  True,\n",
       "        False, False,  True, False, False,  True,  True,  True, False, False,\n",
       "         True, False, False, False,  True, False, False, False, False,  True,\n",
       "         True,  True,  True,  True, False, False,  True,  True,  True, False,\n",
       "        False,  True, False,  True, False,  True, False, False,  True,  True,\n",
       "        False, False,  True,  True], device='cuda:0'), query_features=MoleculeProtoNetFeatures(num_graphs=93, num_nodes=2283, num_edges=2534, node_features=tensor([[0., 0., 1.,  ..., 1., 0., 0.],\n",
       "        [0., 1., 0.,  ..., 1., 0., 1.],\n",
       "        [0., 0., 1.,  ..., 1., 0., 1.],\n",
       "        ...,\n",
       "        [0., 1., 0.,  ..., 1., 0., 1.],\n",
       "        [0., 1., 0.,  ..., 1., 0., 1.],\n",
       "        [0., 1., 0.,  ..., 1., 0., 1.]], device='cuda:0'), adjacency_lists=[tensor([[   0,    1],\n",
       "        [   1,    2],\n",
       "        [   2,    3],\n",
       "        ...,\n",
       "        [2282, 2273],\n",
       "        [2270, 2265],\n",
       "        [2282, 2277]], device='cuda:0'), tensor([[   3,    4],\n",
       "        [   5,    6],\n",
       "        [   7,    8],\n",
       "        ...,\n",
       "        [2277, 2278],\n",
       "        [2279, 2280],\n",
       "        [2281, 2282]], device='cuda:0'), tensor([[ 828,  829],\n",
       "        [ 854,  855],\n",
       "        [1053, 1054],\n",
       "        [1196, 1197],\n",
       "        [1335, 1336],\n",
       "        [1438, 1439],\n",
       "        [1462, 1463],\n",
       "        [1494, 1495],\n",
       "        [1496, 1497],\n",
       "        [1842, 1843],\n",
       "        [1844, 1845],\n",
       "        [2197, 2198]], device='cuda:0')], edge_features=[tensor([], device='cuda:0', size=(1693, 0)), tensor([], device='cuda:0', size=(829, 0)), tensor([], device='cuda:0', size=(12, 0))], node_to_graph=tensor([ 0,  0,  0,  ..., 92, 92, 92], device='cuda:0'), fingerprints=tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 1, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0', dtype=torch.int32), descriptors=tensor([[ 5.6467,  0.4567,  5.6467,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 4.6849,  0.9225,  4.6849,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 5.7432,  0.4511,  5.7432,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        ...,\n",
       "        [ 5.1878,  0.8441,  5.1878,  ...,  1.0000,  0.0000,  0.0000],\n",
       "        [12.0691, -0.1384, 12.0691,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [12.1722, -0.0995, 12.1722,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "       device='cuda:0')))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example.batches[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "\n",
    "from fs_mol.models.protonet import PrototypicalNetworkConfig\n",
    "\n",
    "\n",
    "model = PrototypicalNetwork(config=PrototypicalNetworkConfig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: ProtoNetBatch",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/athecoder/FS-Mol/train_simple.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/athecoder/FS-Mol/train_simple.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/athecoder/FS-Mol/train_simple.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m torch\u001b[39m.\u001b[39;49monnx\u001b[39m.\u001b[39;49mexport(model, example\u001b[39m.\u001b[39;49mbatches[\u001b[39m0\u001b[39;49m], \u001b[39m'\u001b[39;49m\u001b[39mrnn.onnx\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/onnx/__init__.py:350\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[39mExports a model into ONNX format. If ``model`` is not a\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[39m:class:`torch.jit.ScriptModule` nor a :class:`torch.jit.ScriptFunction`, this runs\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[39m    model to the file ``f`` even if this is raised.\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39monnx\u001b[39;00m \u001b[39mimport\u001b[39;00m utils\n\u001b[0;32m--> 350\u001b[0m \u001b[39mreturn\u001b[39;00m utils\u001b[39m.\u001b[39;49mexport(\n\u001b[1;32m    351\u001b[0m     model,\n\u001b[1;32m    352\u001b[0m     args,\n\u001b[1;32m    353\u001b[0m     f,\n\u001b[1;32m    354\u001b[0m     export_params,\n\u001b[1;32m    355\u001b[0m     verbose,\n\u001b[1;32m    356\u001b[0m     training,\n\u001b[1;32m    357\u001b[0m     input_names,\n\u001b[1;32m    358\u001b[0m     output_names,\n\u001b[1;32m    359\u001b[0m     operator_export_type,\n\u001b[1;32m    360\u001b[0m     opset_version,\n\u001b[1;32m    361\u001b[0m     do_constant_folding,\n\u001b[1;32m    362\u001b[0m     dynamic_axes,\n\u001b[1;32m    363\u001b[0m     keep_initializers_as_inputs,\n\u001b[1;32m    364\u001b[0m     custom_opsets,\n\u001b[1;32m    365\u001b[0m     export_modules_as_functions,\n\u001b[1;32m    366\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/onnx/utils.py:163\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexport\u001b[39m(\n\u001b[1;32m    146\u001b[0m     model,\n\u001b[1;32m    147\u001b[0m     args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m     export_modules_as_functions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    161\u001b[0m ):\n\u001b[0;32m--> 163\u001b[0m     _export(\n\u001b[1;32m    164\u001b[0m         model,\n\u001b[1;32m    165\u001b[0m         args,\n\u001b[1;32m    166\u001b[0m         f,\n\u001b[1;32m    167\u001b[0m         export_params,\n\u001b[1;32m    168\u001b[0m         verbose,\n\u001b[1;32m    169\u001b[0m         training,\n\u001b[1;32m    170\u001b[0m         input_names,\n\u001b[1;32m    171\u001b[0m         output_names,\n\u001b[1;32m    172\u001b[0m         operator_export_type\u001b[39m=\u001b[39;49moperator_export_type,\n\u001b[1;32m    173\u001b[0m         opset_version\u001b[39m=\u001b[39;49mopset_version,\n\u001b[1;32m    174\u001b[0m         do_constant_folding\u001b[39m=\u001b[39;49mdo_constant_folding,\n\u001b[1;32m    175\u001b[0m         dynamic_axes\u001b[39m=\u001b[39;49mdynamic_axes,\n\u001b[1;32m    176\u001b[0m         keep_initializers_as_inputs\u001b[39m=\u001b[39;49mkeep_initializers_as_inputs,\n\u001b[1;32m    177\u001b[0m         custom_opsets\u001b[39m=\u001b[39;49mcustom_opsets,\n\u001b[1;32m    178\u001b[0m         export_modules_as_functions\u001b[39m=\u001b[39;49mexport_modules_as_functions,\n\u001b[1;32m    179\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/onnx/utils.py:1074\u001b[0m, in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions)\u001b[0m\n\u001b[1;32m   1071\u001b[0m     dynamic_axes \u001b[39m=\u001b[39m {}\n\u001b[1;32m   1072\u001b[0m _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)\n\u001b[0;32m-> 1074\u001b[0m graph, params_dict, torch_out \u001b[39m=\u001b[39m _model_to_graph(\n\u001b[1;32m   1075\u001b[0m     model,\n\u001b[1;32m   1076\u001b[0m     args,\n\u001b[1;32m   1077\u001b[0m     verbose,\n\u001b[1;32m   1078\u001b[0m     input_names,\n\u001b[1;32m   1079\u001b[0m     output_names,\n\u001b[1;32m   1080\u001b[0m     operator_export_type,\n\u001b[1;32m   1081\u001b[0m     val_do_constant_folding,\n\u001b[1;32m   1082\u001b[0m     fixed_batch_size\u001b[39m=\u001b[39;49mfixed_batch_size,\n\u001b[1;32m   1083\u001b[0m     training\u001b[39m=\u001b[39;49mtraining,\n\u001b[1;32m   1084\u001b[0m     dynamic_axes\u001b[39m=\u001b[39;49mdynamic_axes,\n\u001b[1;32m   1085\u001b[0m )\n\u001b[1;32m   1087\u001b[0m \u001b[39m# TODO: Don't allocate a in-memory string for the protobuf\u001b[39;00m\n\u001b[1;32m   1088\u001b[0m defer_weight_export \u001b[39m=\u001b[39m (\n\u001b[1;32m   1089\u001b[0m     export_type \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39monnx\u001b[39m.\u001b[39mExportTypes\u001b[39m.\u001b[39mPROTOBUF_FILE\n\u001b[1;32m   1090\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/onnx/utils.py:727\u001b[0m, in \u001b[0;36m_model_to_graph\u001b[0;34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001b[0m\n\u001b[1;32m    724\u001b[0m     args \u001b[39m=\u001b[39m (args,)\n\u001b[1;32m    726\u001b[0m model \u001b[39m=\u001b[39m _pre_trace_quant_model(model, args)\n\u001b[0;32m--> 727\u001b[0m graph, params, torch_out, module \u001b[39m=\u001b[39m _create_jit_graph(model, args)\n\u001b[1;32m    728\u001b[0m params_dict \u001b[39m=\u001b[39m _get_named_param_dict(graph, params)\n\u001b[1;32m    730\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/onnx/utils.py:602\u001b[0m, in \u001b[0;36m_create_jit_graph\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m    600\u001b[0m     \u001b[39mreturn\u001b[39;00m graph, params, torch_out, \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    601\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 602\u001b[0m     graph, torch_out \u001b[39m=\u001b[39m _trace_and_get_graph_from_model(model, args)\n\u001b[1;32m    603\u001b[0m     _C\u001b[39m.\u001b[39m_jit_pass_onnx_lint(graph)\n\u001b[1;32m    604\u001b[0m     state_dict \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39m_unique_state_dict(model)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/onnx/utils.py:517\u001b[0m, in \u001b[0;36m_trace_and_get_graph_from_model\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_trace_and_get_graph_from_model\u001b[39m(model, args):\n\u001b[1;32m    513\u001b[0m     \u001b[39m# A basic sanity check: make sure the state_dict keys are the same\u001b[39;00m\n\u001b[1;32m    514\u001b[0m     \u001b[39m# before and after running the model.  Fail fast!\u001b[39;00m\n\u001b[1;32m    515\u001b[0m     orig_state_dict_keys \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39m_unique_state_dict(model)\u001b[39m.\u001b[39mkeys()\n\u001b[0;32m--> 517\u001b[0m     trace_graph, torch_out, inputs_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49m_get_trace_graph(\n\u001b[1;32m    518\u001b[0m         model, args, strict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, _force_outplace\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, _return_inputs_states\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m    519\u001b[0m     )\n\u001b[1;32m    520\u001b[0m     warn_on_static_input_change(inputs_states)\n\u001b[1;32m    522\u001b[0m     \u001b[39mif\u001b[39;00m orig_state_dict_keys \u001b[39m!=\u001b[39m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39m_unique_state_dict(model)\u001b[39m.\u001b[39mkeys():\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/jit/_trace.py:1175\u001b[0m, in \u001b[0;36m_get_trace_graph\u001b[0;34m(f, args, kwargs, strict, _force_outplace, return_inputs, _return_inputs_states)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(args, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m   1174\u001b[0m     args \u001b[39m=\u001b[39m (args,)\n\u001b[0;32m-> 1175\u001b[0m outs \u001b[39m=\u001b[39m ONNXTracedModule(f, strict, _force_outplace, return_inputs, _return_inputs_states)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1176\u001b[0m \u001b[39mreturn\u001b[39;00m outs\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/jit/_trace.py:95\u001b[0m, in \u001b[0;36mONNXTracedModule.forward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m---> 95\u001b[0m     in_vars, in_desc \u001b[39m=\u001b[39m _flatten(args)\n\u001b[1;32m     96\u001b[0m     \u001b[39m# NOTE: use full state, because we need it for BatchNorm export\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     \u001b[39m# This differs from the compiler path, which doesn't support it at the moment.\u001b[39;00m\n\u001b[1;32m     98\u001b[0m     module_state \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(_unique_state_dict(\u001b[39mself\u001b[39m, keep_vars\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mvalues())\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: ProtoNetBatch"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.onnx.export(model, example.batches[0], 'rnn.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "\n",
    "for _ in epochs:\n",
    "    batch = torchify(next(train_task_sample_iterator), device=device)\n",
    "\n",
    "    x = batch.batches\n",
    "    y = batch.batch_labels\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d37b81cb1cce9d4a179366fadc4e97886e60bfa7df3455851b2507fd497d6763"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
